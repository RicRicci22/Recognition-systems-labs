{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTILABEL CLASSIFICATION\n",
    "\n",
    "A multilabel setting is identified by samples that can simultaneously belong to more than one class. For example,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!kaggle datasets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d shivanandmn/multilabel-classification-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf multilabel-classification-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start to explore the dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all, we have to create the dataset\n",
    "from dataset import MultiLabelDataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "dataset = MultiLabelDataset(data_path=\"train.csv\", split='train')\n",
    "# Split the two in train and validation\n",
    "train_dataset, val_dataset = random_split(dataset, [int(len(dataset)*0.9), len(dataset) - int(len(dataset)*0.9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network. We will start with a simple RNN based model. \n",
    "from model import SimpleRNNModel\n",
    "import json\n",
    "\n",
    "word_2_idx = json.load(open('w2i.json'))\n",
    "# HYPER PARAMETERS\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = dataset.__getnlabels__()\n",
    "pad_idx = 0\n",
    "vocab_size = len(word_2_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# TRAIN LOOP \n",
    "# HYPERPARAMETERS\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Model \n",
    "model = SimpleRNNModel(embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_dim=output_dim, vocab_size=vocab_size, pad_idx=pad_idx)\n",
    "# Send the model to the GPU \n",
    "model.to(DEVICE)\n",
    "\n",
    "# Create the dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Create the optimizer\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "# Create the loss function\n",
    "import torch.nn as nn\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils import convert_texts_to_indices\n",
    "\n",
    "# Send the model to the GPU\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        titles, abst, labels = batch\n",
    "        labels = labels.to(DEVICE)\n",
    "        # Prepare the titles\n",
    "        titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "        titles_batch = titles_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(titles_batch)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    print(\"Training loss: {}\".format(round(train_loss/len(train_loader),4)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the validation loss and accuracy \n",
    "\n",
    "BATCH_SIZE = 2\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader):\n",
    "        titles, abst, labels = batch\n",
    "        labels = labels.to(DEVICE)\n",
    "        # Prepare the titles\n",
    "        titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "        titles_batch = titles_batch.to(DEVICE)\n",
    "        out = model(titles_batch)\n",
    "        loss = criterion(out, labels)\n",
    "        val_loss += loss.item()\n",
    "        # Convert the output with sigmoid \n",
    "        out = torch.sigmoid(out)\n",
    "        out = torch.round(out)\n",
    "        # Calculate the accuracy\n",
    "        for i in range(out.size(0)):\n",
    "            if torch.equal(out[i], labels[i]):\n",
    "                correct += 1\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(\"Validation loss: {}\".format(round(val_loss/len(val_loader),4)))\n",
    "print(\"Validation accuracy: {}\".format(round(correct/total,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "target_labels = dataset.labels\n",
    "id2label = {i:label for i, label in enumerate(target_labels)}\n",
    "label2id = {label:i for i, label in enumerate(target_labels)}\n",
    "# Model \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(target_labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import convert_texts_to_indices_bert\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TRAIN LOOP \n",
    "# HYPERPARAMETERS\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "## FREEZE THE WHOLE NETWORK ASIDE THE CLASSIFICATION LAYER\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    if 'classifier' not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "print(\"Model parameters: {}\".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "# Send the model to the GPU \n",
    "model.to(DEVICE)\n",
    "\n",
    "# Create the dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Create the optimizer\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "# Create the loss function\n",
    "import torch.nn as nn\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Send the model to the GPU\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        titles, abst, labels = batch\n",
    "        labels = labels.to(DEVICE)\n",
    "        # Prepare the titles\n",
    "        titles_batch = convert_texts_to_indices_bert(texts=titles,max_len=512)\n",
    "        input_ids = titles_batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = titles_batch['attention_mask'].to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = out.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss+=loss.item()\n",
    "     \n",
    "    print(\"Training loss: {}\".format(round(train_loss/len(train_loader),4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labs2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
