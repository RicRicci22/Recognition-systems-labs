{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTILABEL CLASSIFICATION\n",
    "\n",
    "A multilabel setting is identified by samples that can simultaneously belong to more than one class. For example,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!kaggle datasets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d shivanandmn/multilabel-classification-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf multilabel-classification-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start to explore the dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')\n",
    "df.head()\n",
    "\n",
    "print(df.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all, we have to create the dataset\n",
    "from dataset import MultiLabelDataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "dataset = MultiLabelDataset(data_path=\"train.csv\", split='train')\n",
    "# Split the two in train and validation\n",
    "train_dataset, test_dataset = random_split(dataset, [int(len(dataset)*0.9), len(dataset) - int(len(dataset)*0.9)])\n",
    "train_dataset, val_dataset = random_split(train_dataset, [int(len(train_dataset)*0.9), len(train_dataset) - int(len(train_dataset)*0.9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "word_2_idx = json.load(open('w2i.json'))\n",
    "# Define some quantities\n",
    "output_dim = dataset.__getnlabels__()\n",
    "pad_idx = 0\n",
    "vocab_size = len(word_2_idx)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with the easiest model \n",
    "\n",
    "from model import EmbeddingMatrixModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from tqdm import tqdm\n",
    "from utils import convert_texts_to_indices\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "EMBEDDING_DIM = 100\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cuda:1a' if torch.cuda.is_available() else 'cpu'\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Model \n",
    "model = EmbeddingMatrixModel(embedding_dim=EMBEDDING_DIM, output_dim=output_dim, pad_idx=pad_idx, vocab_size=vocab_size)\n",
    "# Send the model to the GPU \n",
    "model.to(DEVICE)\n",
    "\n",
    "# Create the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "# Create the loss function\n",
    "criterion = BCEWithLogitsLoss()\n",
    "\n",
    "# Send the model to the GPU\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        titles, abst, labels = batch\n",
    "        labels = labels.to(DEVICE)\n",
    "        # Prepare the titles\n",
    "        titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "        titles_batch = titles_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out, loss_mask = model(titles_batch)\n",
    "        out = out[loss_mask.squeeze()==1]\n",
    "        labels = labels[loss_mask.squeeze()==1]\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # EVALUATE ON THE VALIDATION SPLIT \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            titles, abst, labels = batch\n",
    "            labels = labels.to(DEVICE)\n",
    "            # Prepare the titles\n",
    "            titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "            titles_batch = titles_batch.to(DEVICE)\n",
    "            out, loss_mask = model(titles_batch)\n",
    "            out = out[loss_mask.squeeze()==1]\n",
    "            labels = labels[loss_mask.squeeze()==1]\n",
    "            loss = criterion(out, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "        \n",
    "    print(\"Training loss epoch {}: {}\".format(epoch, round(train_loss/len(train_loader),4)))\n",
    "    print(\"Validation loss epoch {}: {}\".format(epoch, round(val_loss/len(val_loader),4)))\n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "    val_losses.append(val_loss/len(val_loader))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot val losses and train losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the test loss and accuracy \n",
    "\n",
    "BATCH_SIZE = 128\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        titles, abst, labels = batch\n",
    "        labels = labels.to(DEVICE)\n",
    "        # Prepare the titles\n",
    "        titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "        titles_batch = titles_batch.to(DEVICE)\n",
    "        out, loss_mask = model(titles_batch)\n",
    "        loss = criterion(out, labels)\n",
    "        val_loss += loss.item()\n",
    "        # Convert the output with sigmoid \n",
    "        out = torch.sigmoid(out)\n",
    "        out = torch.round(out)\n",
    "        # Calculate the accuracy\n",
    "        for i in range(out.size(0)):\n",
    "            if torch.equal(out[i], labels[i]):\n",
    "                correct += 1\n",
    "        total += labels.size(0)\n",
    "\n",
    "        test_predictions.extend(out.tolist())\n",
    "        test_labels.extend(labels.tolist())\n",
    "\n",
    "print(\"Validation loss: {}\".format(round(val_loss/len(test_loader),4)))\n",
    "print(\"Validation accuracy: {}\".format(round(correct/total,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "test_labels = np.array(test_labels)\n",
    "test_predictions = np.array(test_predictions)\n",
    "report = classification_report(test_labels,test_predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with the easiest model \n",
    "\n",
    "from model import EmbeddingMatrixModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from tqdm import tqdm\n",
    "from utils import convert_texts_to_indices\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "EMBEDDING_DIM = 100\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Model \n",
    "model = EmbeddingMatrixModel(embedding_dim=EMBEDDING_DIM, output_dim=output_dim, pad_idx=pad_idx, vocab_size=vocab_size)\n",
    "# Send the model to the GPU \n",
    "model.to(DEVICE)\n",
    "\n",
    "# Create the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE, weight_decay=1e-3)\n",
    "\n",
    "# Create the loss function\n",
    "criterion = BCEWithLogitsLoss()\n",
    "\n",
    "# Send the model to the GPU\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        titles, abst, labels = batch\n",
    "        labels = labels.to(DEVICE)\n",
    "        # Prepare the titles\n",
    "        titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "        titles_batch = titles_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out, loss_mask = model(titles_batch)\n",
    "        out = out[loss_mask.squeeze()==1]\n",
    "        labels = labels[loss_mask.squeeze()==1]\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # EVALUATE ON THE VALIDATION SPLIT \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            titles, abst, labels = batch\n",
    "            labels = labels.to(DEVICE)\n",
    "            # Prepare the titles\n",
    "            titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "            titles_batch = titles_batch.to(DEVICE)\n",
    "            out, loss_mask = model(titles_batch)\n",
    "            out = out[loss_mask.squeeze()==1]\n",
    "            labels = labels[loss_mask.squeeze()==1]\n",
    "            loss = criterion(out, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "        \n",
    "    print(\"Training loss epoch {}: {}\".format(epoch, round(train_loss/len(train_loader),4)))\n",
    "    print(\"Validation loss epoch {}: {}\".format(epoch, round(val_loss/len(val_loader),4)))\n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "    val_losses.append(val_loss/len(val_loader))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot val losses and train losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the test loss and accuracy \n",
    "\n",
    "BATCH_SIZE = 128\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        titles, abst, labels = batch\n",
    "        labels = labels.to(DEVICE)\n",
    "        # Prepare the titles\n",
    "        titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "        titles_batch = titles_batch.to(DEVICE)\n",
    "        out, loss_mask = model(titles_batch)\n",
    "        loss = criterion(out, labels)\n",
    "        val_loss += loss.item()\n",
    "        # Convert the output with sigmoid \n",
    "        out = torch.sigmoid(out)\n",
    "        out = torch.round(out)\n",
    "        # Calculate the accuracy\n",
    "        for i in range(out.size(0)):\n",
    "            if torch.equal(out[i], labels[i]):\n",
    "                correct += 1\n",
    "        total += labels.size(0)\n",
    "\n",
    "        test_predictions.extend(out.tolist())\n",
    "        test_labels.extend(labels.tolist())\n",
    "\n",
    "print(\"Validation loss: {}\".format(round(val_loss/len(test_loader),4)))\n",
    "print(\"Validation accuracy: {}\".format(round(correct/total,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "test_labels = np.array(test_labels)\n",
    "test_predictions = np.array(test_predictions)\n",
    "report = classification_report(test_labels,test_predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a step, use a RNN based model. \n",
    "from model import SimpleRNNModel\n",
    "import json\n",
    "\n",
    "word_2_idx = json.load(open('w2i.json'))\n",
    "# HYPER PARAMETERS\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = dataset.__getnlabels__()\n",
    "pad_idx = 0\n",
    "vocab_size = len(word_2_idx)+1\n",
    "\n",
    "print(embedding_dim)\n",
    "print(hidden_dim)\n",
    "print(output_dim)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import SimpleRNNModel\n",
    "# TRAIN LOOP \n",
    "# HYPERPARAMETERS\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Model \n",
    "model = SimpleRNNModel(embedding_dim=embedding_dim, \n",
    "                       hidden_dim=hidden_dim, \n",
    "                       output_dim=output_dim, \n",
    "                       vocab_size=vocab_size, \n",
    "                       pad_idx=pad_idx)\n",
    "# Send the model to the GPU \n",
    "model.to(DEVICE)\n",
    "\n",
    "# Create the dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Create the optimizer\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE, weight_decay=1e-3)\n",
    "\n",
    "# Create the loss function\n",
    "import torch.nn as nn\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils import convert_texts_to_indices\n",
    "\n",
    "# Send the model to the GPU\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        titles, abst, labels = batch\n",
    "        labels = labels.to(DEVICE)\n",
    "        # Prepare the titles\n",
    "        titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "        titles_batch = titles_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(titles_batch)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # EVALUATE ON THE VALIDATION SPLIT \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            titles, abst, labels = batch\n",
    "            labels = labels.to(DEVICE)\n",
    "            # Prepare the titles\n",
    "            titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "            titles_batch = titles_batch.to(DEVICE)\n",
    "            out = model(titles_batch)\n",
    "            loss = criterion(out, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    print(\"Training loss epoch {}: {}\".format(epoch, round(train_loss/len(train_loader),4)))\n",
    "    print(\"Validation loss epoch {}: {}\".format(epoch, round(val_loss/len(val_loader),4)))\n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "    val_losses.append(val_loss/len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the test loss and accuracy \n",
    "\n",
    "BATCH_SIZE = 128\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        titles, abst, labels = batch\n",
    "        labels = labels.to(DEVICE)\n",
    "        # Prepare the titles\n",
    "        titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "        titles_batch = titles_batch.to(DEVICE)\n",
    "        out = model(titles_batch)\n",
    "        loss = criterion(out, labels)\n",
    "        val_loss += loss.item()\n",
    "        # Convert the output with sigmoid \n",
    "        out = torch.sigmoid(out)\n",
    "        out = torch.round(out)\n",
    "        # Calculate the accuracy\n",
    "        for i in range(out.size(0)):\n",
    "            if torch.equal(out[i], labels[i]):\n",
    "                correct += 1\n",
    "        total += labels.size(0)\n",
    "\n",
    "        test_predictions.extend(out.tolist())\n",
    "        test_labels.extend(labels.tolist())\n",
    "\n",
    "print(\"Validation loss: {}\".format(round(val_loss/len(test_loader),4)))\n",
    "print(\"Validation accuracy: {}\".format(round(correct/total,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "test_labels = np.array(test_labels)\n",
    "test_predictions = np.array(test_predictions)\n",
    "report = classification_report(test_labels,test_predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from dataset import BERT_dataset\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# FIX THE SEED\n",
    "random.seed(45)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "dataset = BERT_dataset(data_path=\"train.csv\", split='train')\n",
    "# Split the two in train and validation\n",
    "train_dataset, test_dataset = random_split(dataset, [int(len(dataset)*0.9), len(dataset) - int(len(dataset)*0.9)])\n",
    "train_dataset, val_dataset = random_split(train_dataset, [int(len(train_dataset)*0.9), len(train_dataset) - int(len(train_dataset)*0.9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = dataset.labels\n",
    "id2label = {i:label for i, label in enumerate(target_labels)}\n",
    "label2id = {label:i for i, label in enumerate(target_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from dataset import custom_collate\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# Model \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(target_labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"classifier\" not in name:\n",
    "#         param.requires_grad = False\n",
    "        \n",
    "print(\"Model parameters: {}\".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "# Send the model to the GPU \n",
    "model.to(DEVICE)\n",
    "\n",
    "# Create the dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=8, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=8, collate_fn=custom_collate)\n",
    "\n",
    "# Create the optimizer\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "# Create the loss function\n",
    "import torch.nn as nn\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Send the model to the GPU\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        labels = labels.to(DEVICE)\n",
    "        input_ids = input_ids.to(DEVICE)\n",
    "        attention_mask = attention_mask.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = out.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss+=loss.item()\n",
    "    \n",
    "    # EVALUATE ON THE VALIDATION SPLIT \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            labels = labels.to(DEVICE)\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = out.loss\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    print(\"Training loss epoch {}: {}\".format(epoch, round(train_loss/len(train_loader),4)))\n",
    "    print(\"Validation loss epoch {}: {}\".format(epoch, round(val_loss/len(val_loader),4)))\n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "    val_losses.append(val_loss/len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the test loss and accuracy \n",
    "\n",
    "BATCH_SIZE = 128\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, collate_fn=custom_collate)\n",
    "\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        labels = labels.to(DEVICE)\n",
    "        input_ids = input_ids.to(DEVICE)\n",
    "        attention_mask = attention_mask.to(DEVICE)\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = out.loss\n",
    "        val_loss += loss.item()\n",
    "        # Convert the output with sigmoid \n",
    "        out = torch.sigmoid(out[\"logits\"])\n",
    "        out = torch.round(out)\n",
    "        # Calculate the accuracy\n",
    "        for i in range(out.size(0)):\n",
    "            if torch.equal(out[i], labels[i]):\n",
    "                correct += 1\n",
    "        total += labels.size(0)\n",
    "\n",
    "        test_predictions.extend(out.tolist())\n",
    "        test_labels.extend(labels.tolist())\n",
    "\n",
    "print(\"Validation loss: {}\".format(round(val_loss/len(test_loader),4)))\n",
    "print(\"Validation accuracy: {}\".format(round(correct/total,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "test_labels = np.array(test_labels)\n",
    "test_predictions = np.array(test_predictions)\n",
    "report = classification_report(test_labels,test_predictions)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labs2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
