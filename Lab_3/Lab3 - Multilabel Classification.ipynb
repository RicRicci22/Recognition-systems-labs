{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTILABEL CLASSIFICATION\n",
    "\n",
    "A multilabel setting is identified by samples that can simultaneously belong to more than one class. For example,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!kaggle datasets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d shivanandmn/multilabel-classification-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf multilabel-classification-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start to explore the dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')\n",
    "df.head()\n",
    "\n",
    "print(df.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all, we have to create the dataset\n",
    "from dataset import MultiLabelDataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "dataset = MultiLabelDataset(data_path=\"train.csv\", split='train')\n",
    "# Split the two in train and validation\n",
    "train_dataset, test_dataset = random_split(dataset, [int(len(dataset)*0.9), len(dataset) - int(len(dataset)*0.9)])\n",
    "train_dataset, val_dataset = random_split(train_dataset, [int(len(train_dataset)*0.9), len(train_dataset) - int(len(train_dataset)*0.9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "word_2_idx = json.load(open('w2i.json'))\n",
    "# Define some quantities\n",
    "output_dim = dataset.__getnlabels__()\n",
    "pad_idx = 0\n",
    "vocab_size = len(word_2_idx)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with the easiest model \n",
    "\n",
    "from model import EmbeddingMatrixModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from tqdm import tqdm\n",
    "from utils import convert_texts_to_indices\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "EMBEDDING_DIM = 100\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Model \n",
    "model = EmbeddingMatrixModel(embedding_dim=EMBEDDING_DIM, output_dim=output_dim, pad_idx=pad_idx, vocab_size=vocab_size)\n",
    "# Send the model to the GPU \n",
    "model.to(DEVICE)\n",
    "\n",
    "# Create the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "# Create the loss function\n",
    "criterion = BCEWithLogitsLoss()\n",
    "\n",
    "# Send the model to the GPU\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        titles, abst, labels = batch\n",
    "        labels = labels.to(DEVICE)\n",
    "        # Prepare the titles\n",
    "        titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "        titles_batch = titles_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out, loss_mask = model(titles_batch)\n",
    "        out = out[loss_mask.squeeze()==1]\n",
    "        labels = labels[loss_mask.squeeze()==1]\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # EVALUATE ON THE VALIDATION SPLIT \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            titles, abst, labels = batch\n",
    "            labels = labels.to(DEVICE)\n",
    "            # Prepare the titles\n",
    "            titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "            titles_batch = titles_batch.to(DEVICE)\n",
    "            out, loss_mask = model(titles_batch)\n",
    "            out = out[loss_mask.squeeze()==1]\n",
    "            labels = labels[loss_mask.squeeze()==1]\n",
    "            loss = criterion(out, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "        \n",
    "    print(\"Training loss epoch {}: {}\".format(epoch, round(train_loss/len(train_loader),4)))\n",
    "    print(\"Validation loss epoch {}: {}\".format(epoch, round(val_loss/len(val_loader),4)))\n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "    val_losses.append(val_loss/len(val_loader))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot val losses and train losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the test loss and accuracy \n",
    "\n",
    "BATCH_SIZE = 128\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        titles, abst, labels = batch\n",
    "        labels = labels.to(DEVICE)\n",
    "        # Prepare the titles\n",
    "        titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "        titles_batch = titles_batch.to(DEVICE)\n",
    "        out, loss_mask = model(titles_batch)\n",
    "        loss = criterion(out, labels)\n",
    "        val_loss += loss.item()\n",
    "        # Convert the output with sigmoid \n",
    "        out = torch.sigmoid(out)\n",
    "        out = torch.round(out)\n",
    "        # Calculate the accuracy\n",
    "        for i in range(out.size(0)):\n",
    "            if torch.equal(out[i], labels[i]):\n",
    "                correct += 1\n",
    "        total += labels.size(0)\n",
    "\n",
    "        test_predictions.extend(out.tolist())\n",
    "        test_labels.extend(labels.tolist())\n",
    "\n",
    "print(\"Validation loss: {}\".format(round(val_loss/len(test_loader),4)))\n",
    "print(\"Validation accuracy: {}\".format(round(correct/total,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "test_labels = np.array(test_labels)\n",
    "test_predictions = np.array(test_predictions)\n",
    "report = classification_report(test_labels,test_predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with the easiest model \n",
    "\n",
    "from model import EmbeddingMatrixModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from tqdm import tqdm\n",
    "from utils import convert_texts_to_indices\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "EMBEDDING_DIM = 100\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Model \n",
    "model = EmbeddingMatrixModel(embedding_dim=EMBEDDING_DIM, output_dim=output_dim, pad_idx=pad_idx, vocab_size=vocab_size)\n",
    "# Send the model to the GPU \n",
    "model.to(DEVICE)\n",
    "\n",
    "# Create the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE, weight_decay=1e-3)\n",
    "\n",
    "# Create the loss function\n",
    "criterion = BCEWithLogitsLoss()\n",
    "\n",
    "# Send the model to the GPU\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        titles, abst, labels = batch\n",
    "        labels = labels.to(DEVICE)\n",
    "        # Prepare the titles\n",
    "        titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "        titles_batch = titles_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out, loss_mask = model(titles_batch)\n",
    "        out = out[loss_mask.squeeze()==1]\n",
    "        labels = labels[loss_mask.squeeze()==1]\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # EVALUATE ON THE VALIDATION SPLIT \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            titles, abst, labels = batch\n",
    "            labels = labels.to(DEVICE)\n",
    "            # Prepare the titles\n",
    "            titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "            titles_batch = titles_batch.to(DEVICE)\n",
    "            out, loss_mask = model(titles_batch)\n",
    "            out = out[loss_mask.squeeze()==1]\n",
    "            labels = labels[loss_mask.squeeze()==1]\n",
    "            loss = criterion(out, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "        \n",
    "    print(\"Training loss epoch {}: {}\".format(epoch, round(train_loss/len(train_loader),4)))\n",
    "    print(\"Validation loss epoch {}: {}\".format(epoch, round(val_loss/len(val_loader),4)))\n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "    val_losses.append(val_loss/len(val_loader))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot val losses and train losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:00<00:00, 17.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.252\n",
      "Validation accuracy: 0.5639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the test loss and accuracy \n",
    "\n",
    "BATCH_SIZE = 128\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        titles, abst, labels = batch\n",
    "        labels = labels.to(DEVICE)\n",
    "        # Prepare the titles\n",
    "        titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "        titles_batch = titles_batch.to(DEVICE)\n",
    "        out, loss_mask = model(titles_batch)\n",
    "        loss = criterion(out, labels)\n",
    "        val_loss += loss.item()\n",
    "        # Convert the output with sigmoid \n",
    "        out = torch.sigmoid(out)\n",
    "        out = torch.round(out)\n",
    "        # Calculate the accuracy\n",
    "        for i in range(out.size(0)):\n",
    "            if torch.equal(out[i], labels[i]):\n",
    "                correct += 1\n",
    "        total += labels.size(0)\n",
    "\n",
    "        test_predictions.extend(out.tolist())\n",
    "        test_labels.extend(labels.tolist())\n",
    "\n",
    "print(\"Validation loss: {}\".format(round(val_loss/len(test_loader),4)))\n",
    "print(\"Validation accuracy: {}\".format(round(correct/total,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.76      0.78       889\n",
      "           1       0.84      0.76      0.80       579\n",
      "           2       0.77      0.72      0.74       561\n",
      "           3       0.72      0.64      0.68       535\n",
      "           4       0.39      0.12      0.18        60\n",
      "           5       0.29      0.10      0.14        21\n",
      "\n",
      "   micro avg       0.78      0.71      0.74      2645\n",
      "   macro avg       0.63      0.52      0.55      2645\n",
      "weighted avg       0.77      0.71      0.74      2645\n",
      " samples avg       0.75      0.74      0.73      2645\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Riccardo\\miniconda3\\envs\\labs2024\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "test_labels = np.array(test_labels)\n",
    "test_predictions = np.array(test_predictions)\n",
    "report = classification_report(test_labels,test_predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "256\n",
      "6\n",
      "4503\n"
     ]
    }
   ],
   "source": [
    "# Do a step, use a RNN based model.\n",
    "import json\n",
    "\n",
    "word_2_idx = json.load(open('w2i.json'))\n",
    "# HYPER PARAMETERS\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = dataset.__getnlabels__()\n",
    "pad_idx = 0\n",
    "vocab_size = len(word_2_idx)+1\n",
    "\n",
    "print(embedding_dim)\n",
    "print(hidden_dim)\n",
    "print(output_dim)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 531/531 [00:11<00:00, 48.15it/s]\n",
      "100%|██████████| 59/59 [00:00<00:00, 99.44it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch 0: 0.4242\n",
      "Validation loss epoch 0: 0.403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 531/531 [00:10<00:00, 48.68it/s]\n",
      "100%|██████████| 59/59 [00:00<00:00, 101.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch 1: 0.3981\n",
      "Validation loss epoch 1: 0.3921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 531/531 [00:10<00:00, 49.77it/s]\n",
      "100%|██████████| 59/59 [00:00<00:00, 104.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch 2: 0.3816\n",
      "Validation loss epoch 2: 0.3754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 531/531 [00:10<00:00, 49.41it/s]\n",
      "100%|██████████| 59/59 [00:00<00:00, 99.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch 3: 0.3582\n",
      "Validation loss epoch 3: 0.346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 531/531 [00:10<00:00, 49.94it/s]\n",
      "100%|██████████| 59/59 [00:00<00:00, 97.58it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch 4: 0.3321\n",
      "Validation loss epoch 4: 0.3368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 531/531 [00:11<00:00, 47.49it/s]\n",
      "100%|██████████| 59/59 [00:00<00:00, 99.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch 5: 0.314\n",
      "Validation loss epoch 5: 0.3145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 531/531 [00:11<00:00, 47.62it/s]\n",
      "100%|██████████| 59/59 [00:00<00:00, 96.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch 6: 0.3029\n",
      "Validation loss epoch 6: 0.3118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 531/531 [00:11<00:00, 44.79it/s]\n",
      "100%|██████████| 59/59 [00:01<00:00, 50.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch 7: 0.2989\n",
      "Validation loss epoch 7: 0.3057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 531/531 [00:14<00:00, 36.02it/s]\n",
      "100%|██████████| 59/59 [00:00<00:00, 106.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch 8: 0.2933\n",
      "Validation loss epoch 8: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 531/531 [00:10<00:00, 50.89it/s]\n",
      "100%|██████████| 59/59 [00:00<00:00, 95.37it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss epoch 9: 0.2908\n",
      "Validation loss epoch 9: 0.3043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model import SimpleRNNModel\n",
    "# TRAIN LOOP \n",
    "# HYPERPARAMETERS\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Model \n",
    "model = SimpleRNNModel(embedding_dim=embedding_dim, \n",
    "                       hidden_dim=hidden_dim, \n",
    "                       output_dim=output_dim, \n",
    "                       vocab_size=vocab_size, \n",
    "                       pad_idx=pad_idx)\n",
    "\n",
    "# Send the model to the GPU \n",
    "model.to(DEVICE)\n",
    "\n",
    "# Create the dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Create the optimizer\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE, weight_decay=1e-3)\n",
    "\n",
    "# Create the loss function\n",
    "import torch.nn as nn\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils import convert_texts_to_indices\n",
    "\n",
    "# Send the model to the GPU\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        titles, abst, labels = batch\n",
    "        labels = labels.to(DEVICE)\n",
    "        # Prepare the titles\n",
    "        titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "        titles_batch = titles_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(titles_batch)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # EVALUATE ON THE VALIDATION SPLIT \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            titles, abst, labels = batch\n",
    "            labels = labels.to(DEVICE)\n",
    "            # Prepare the titles\n",
    "            titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "            titles_batch = titles_batch.to(DEVICE)\n",
    "            out = model(titles_batch)\n",
    "            loss = criterion(out, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    print(\"Training loss epoch {}: {}\".format(epoch, round(train_loss/len(train_loader),4)))\n",
    "    print(\"Validation loss epoch {}: {}\".format(epoch, round(val_loss/len(val_loader),4)))\n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "    val_losses.append(val_loss/len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 15.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2954\n",
      "Validation accuracy: 0.4743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the test loss and accuracy \n",
    "\n",
    "BATCH_SIZE = 128\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        titles, abst, labels = batch\n",
    "        labels = labels.to(DEVICE)\n",
    "        # Prepare the titles\n",
    "        titles_batch = convert_texts_to_indices(texts=titles,word2idx=word_2_idx,pad_idx=pad_idx)\n",
    "        titles_batch = titles_batch.to(DEVICE)\n",
    "        out = model(titles_batch)\n",
    "        loss = criterion(out, labels)\n",
    "        val_loss += loss.item()\n",
    "        # Convert the output with sigmoid \n",
    "        out = torch.sigmoid(out)\n",
    "        out = torch.round(out)\n",
    "        # Calculate the accuracy\n",
    "        for i in range(out.size(0)):\n",
    "            if torch.equal(out[i], labels[i]):\n",
    "                correct += 1\n",
    "        total += labels.size(0)\n",
    "\n",
    "        test_predictions.extend(out.tolist())\n",
    "        test_labels.extend(labels.tolist())\n",
    "\n",
    "print(\"Validation loss: {}\".format(round(val_loss/len(test_loader),4)))\n",
    "print(\"Validation accuracy: {}\".format(round(correct/total,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.75      0.74       889\n",
      "           1       0.83      0.67      0.74       579\n",
      "           2       0.82      0.52      0.63       561\n",
      "           3       0.67      0.42      0.52       535\n",
      "           4       0.00      0.00      0.00        60\n",
      "           5       0.00      0.00      0.00        21\n",
      "\n",
      "   micro avg       0.76      0.59      0.67      2645\n",
      "   macro avg       0.51      0.39      0.44      2645\n",
      "weighted avg       0.74      0.59      0.65      2645\n",
      " samples avg       0.63      0.62      0.61      2645\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Riccardo\\miniconda3\\envs\\labs2024\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Riccardo\\miniconda3\\envs\\labs2024\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "test_labels = np.array(test_labels)\n",
    "test_predictions = np.array(test_predictions)\n",
    "report = classification_report(test_labels,test_predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully, total number of train samples: 20972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Riccardo\\miniconda3\\envs\\labs2024\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "from dataset import BERT_dataset\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# FIX THE SEED\n",
    "random.seed(45)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "dataset = BERT_dataset(data_path=\"train.csv\", split='train')\n",
    "# Split the two in train and validation\n",
    "train_dataset, test_dataset = random_split(dataset, [int(len(dataset)*0.9), len(dataset) - int(len(dataset)*0.9)])\n",
    "train_dataset, val_dataset = random_split(train_dataset, [int(len(train_dataset)*0.9), len(train_dataset) - int(len(train_dataset)*0.9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = dataset.labels\n",
    "id2label = {i:label for i, label in enumerate(target_labels)}\n",
    "label2id = {label:i for i, label in enumerate(target_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Riccardo\\miniconda3\\envs\\labs2024\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 66958086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/531 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m     49\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m     51\u001b[0m         input_ids, attention_mask, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m     52\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\miniconda3\\envs\\labs2024\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\miniconda3\\envs\\labs2024\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\miniconda3\\envs\\labs2024\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\miniconda3\\envs\\labs2024\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\miniconda3\\envs\\labs2024\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\miniconda3\\envs\\labs2024\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\miniconda3\\envs\\labs2024\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\miniconda3\\envs\\labs2024\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Riccardo\\miniconda3\\envs\\labs2024\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from dataset import custom_collate\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# Model \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(target_labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"classifier\" not in name:\n",
    "#         param.requires_grad = False\n",
    "        \n",
    "print(\"Model parameters: {}\".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "# Send the model to the GPU \n",
    "model.to(DEVICE)\n",
    "\n",
    "# Create the dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=8, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=8, collate_fn=custom_collate)\n",
    "\n",
    "# Create the optimizer\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "# Create the loss function\n",
    "import torch.nn as nn\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Send the model to the GPU\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        labels = labels.to(DEVICE)\n",
    "        input_ids = input_ids.to(DEVICE)\n",
    "        attention_mask = attention_mask.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = out.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss+=loss.item()\n",
    "    \n",
    "    # EVALUATE ON THE VALIDATION SPLIT \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            labels = labels.to(DEVICE)\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = out.loss\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    print(\"Training loss epoch {}: {}\".format(epoch, round(train_loss/len(train_loader),4)))\n",
    "    print(\"Validation loss epoch {}: {}\".format(epoch, round(val_loss/len(val_loader),4)))\n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "    val_losses.append(val_loss/len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the test loss and accuracy \n",
    "\n",
    "BATCH_SIZE = 128\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, collate_fn=custom_collate)\n",
    "\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        labels = labels.to(DEVICE)\n",
    "        input_ids = input_ids.to(DEVICE)\n",
    "        attention_mask = attention_mask.to(DEVICE)\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = out.loss\n",
    "        val_loss += loss.item()\n",
    "        # Convert the output with sigmoid \n",
    "        out = torch.sigmoid(out[\"logits\"])\n",
    "        out = torch.round(out)\n",
    "        # Calculate the accuracy\n",
    "        for i in range(out.size(0)):\n",
    "            if torch.equal(out[i], labels[i]):\n",
    "                correct += 1\n",
    "        total += labels.size(0)\n",
    "\n",
    "        test_predictions.extend(out.tolist())\n",
    "        test_labels.extend(labels.tolist())\n",
    "\n",
    "print(\"Validation loss: {}\".format(round(val_loss/len(test_loader),4)))\n",
    "print(\"Validation accuracy: {}\".format(round(correct/total,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "test_labels = np.array(test_labels)\n",
    "test_predictions = np.array(test_predictions)\n",
    "report = classification_report(test_labels,test_predictions)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labs2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
