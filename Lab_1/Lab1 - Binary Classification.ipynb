{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":91},"executionInfo":{"elapsed":42522,"status":"ok","timestamp":1677660513319,"user":{"displayName":"Riccardo Ricci","userId":"11679554425713061537"},"user_tz":-60},"id":"pBgg15IFlkU2","outputId":"b0f90efe-9f4f-43d5-d1a3-d6a4e8aa7cf8"},"outputs":[],"source":["# from google.colab import files\n","# files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11179,"status":"ok","timestamp":1677596762990,"user":{"displayName":"Riccardo Ricci","userId":"11679554425713061537"},"user_tz":-60},"id":"5cQAp4686j0b","outputId":"0ee78d01-8c2e-4980-f2a1-e5116bc99fd4"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1677660517474,"user":{"displayName":"Riccardo Ricci","userId":"11679554425713061537"},"user_tz":-60},"id":"j04ZKPJSmeyK"},"outputs":[],"source":["# !mkdir ~/.kaggle\n","# !cp kaggle.json ~/.kaggle/\n","# !chmod 600 ~/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":437,"status":"ok","timestamp":1677660520150,"user":{"displayName":"Riccardo Ricci","userId":"11679554425713061537"},"user_tz":-60},"id":"Qi3PLeMHmtrt","outputId":"a957672d-311f-41a7-f459-235be0f1d74e"},"outputs":[{"name":"stdout","output_type":"stream","text":["ref                                                                                deadline             category             reward  teamCount  userHasEntered  \n","---------------------------------------------------------------------------------  -------------------  ---------------  ----------  ---------  --------------  \n","https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize                 2024-06-27 23:59:00  Featured         $1,048,576        462           False  \n","https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability        2024-05-27 23:59:00  Featured           $105,000       3083           False  \n","https://www.kaggle.com/competitions/lmsys-chatbot-arena                            2024-08-05 23:59:00  Research           $100,000        157           False  \n","https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2  2024-07-02 23:59:00  Featured            $50,000       1214           False  \n","https://www.kaggle.com/competitions/leash-BELKA                                    2024-07-08 23:59:00  Featured            $50,000        753           False  \n","https://www.kaggle.com/competitions/image-matching-challenge-2024                  2024-06-03 23:59:00  Research            $50,000        721           False  \n","https://www.kaggle.com/competitions/birdclef-2024                                  2024-06-10 23:59:00  Research            $50,000        552           False  \n","https://www.kaggle.com/competitions/leap-atmospheric-physics-ai-climsim            2024-07-01 23:59:00  Research            $50,000        268           False  \n","https://www.kaggle.com/competitions/uspto-explainable-ai                           2024-07-24 23:59:00  Featured            $50,000         60           False  \n","https://www.kaggle.com/competitions/playground-series-s4e5                         2024-05-31 23:59:00  Playground             Swag        859           False  \n","https://www.kaggle.com/competitions/planttraits2024                                2024-06-15 22:00:00  Research          Knowledge        268           False  \n","https://www.kaggle.com/competitions/geolifeclef-2024                               2024-05-24 23:59:00  Research          Knowledge         76           False  \n","https://www.kaggle.com/competitions/titanic                                        2030-01-01 00:00:00  Getting Started   Knowledge      15628           False  \n","https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques    2030-01-01 00:00:00  Getting Started   Knowledge       4680           False  \n","https://www.kaggle.com/competitions/spaceship-titanic                              2030-01-01 00:00:00  Getting Started   Knowledge       2531           False  \n","https://www.kaggle.com/competitions/digit-recognizer                               2030-01-01 00:00:00  Getting Started   Knowledge       1837           False  \n","https://www.kaggle.com/competitions/nlp-getting-started                            2030-01-01 00:00:00  Getting Started   Knowledge       1144           False  \n","https://www.kaggle.com/competitions/store-sales-time-series-forecasting            2030-06-30 23:59:00  Getting Started   Knowledge        715           False  \n","https://www.kaggle.com/competitions/connectx                                       2030-01-01 00:00:00  Getting Started   Knowledge        225           False  \n","https://www.kaggle.com/competitions/gan-getting-started                            2030-07-01 23:59:00  Getting Started   Knowledge        126           False  \n"]}],"source":["!kaggle competitions list"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37022,"status":"ok","timestamp":1677660559088,"user":{"displayName":"Riccardo Ricci","userId":"11679554425713061537"},"user_tz":-60},"id":"VwdHKFN5jF9l","outputId":"9847690e-0e32-4458-9fd7-34589d3e3e2e"},"outputs":[],"source":["# Download the dataset\n","!kaggle competitions download -c dogs-vs-cats-redux-kernels-edition"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19810,"status":"ok","timestamp":1677660578876,"user":{"displayName":"Riccardo Ricci","userId":"11679554425713061537"},"user_tz":-60},"id":"s9jKI4qAjXVn","outputId":"4ec09bff-6f12-4134-8a47-ff8f31e95e90"},"outputs":[],"source":["# Unzip the dataset\n","!tar -xf dogs-vs-cats-redux-kernels-edition.zip\n","!tar -xf test.zip\n","!tar -xf train.zip"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**DATA EXPLORATION**\n","\n","Let's have a look at the images in the folders, to get an idea of the data that we have."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1677660578877,"user":{"displayName":"Riccardo Ricci","userId":"11679554425713061537"},"user_tz":-60},"id":"IHUUKC3HptEW"},"outputs":[],"source":["# Import modules\n","import os\n","from PIL import Image\n","import random\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Number of training images: \",len(os.listdir(\"train\")))\n","print(\"Number of testing images: \",len(os.listdir(\"test\")))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":862},"executionInfo":{"elapsed":8435,"status":"ok","timestamp":1677660599389,"user":{"displayName":"Riccardo Ricci","userId":"11679554425713061537"},"user_tz":-60},"id":"sKwkyeobppL7","outputId":"cac40e97-755c-4d1f-a3dd-3f5385f1a1f0"},"outputs":[],"source":["# Explore the data in the train folder\n","\n","files_train = os.listdir(\"train\")\n","random.shuffle(files_train)\n","fig = plt.figure(figsize=(15,15))\n","num_images = 16\n","for num, fn in enumerate(files_train[:num_images]):\n","  path = os.path.join(\"train\",fn)\n","  img = Image.open(path)\n","  plt.subplot(num_images//4,4,num+1)\n","  plt.title(fn)\n","  plt.axis('off')\n","  plt.imshow(img)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Explore the data in the test folder\n","\n","files_train = os.listdir(\"test\")\n","random.shuffle(files_train)\n","fig = plt.figure(figsize=(15,15))\n","num_images = 16\n","for num, fn in enumerate(files_train[:num_images]):\n","  path = os.path.join(\"test\",fn)\n","  img = Image.open(path)\n","  plt.subplot(num_images//4,4,num+1)\n","  plt.title(fn)\n","  plt.axis('off')\n","  plt.imshow(img)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# CODE PART 1: BUILDING AND TRAINING PLAIN-CNN\n","\n","We will build and train a plain convolutional neural network without skip-connections, following the architecture described in the paper by\n","He et al.\n","\n","Here is the schematic depiction of the network. As you can see, there are mainly 5 \"blocks\" of convolutional layers. Each block contains a variable number of layers, specified by a configuration list. There is one max pooling layer just after the first block, and an average pooling layer at the end before the fully connected layer. The kernel size is specified for each block, and is the same for each layer inside the block. \n","Furthermore, the output channel and stride are also specified for each block.\n","\n","![images_notebook/plainCNN.jpg](images_notebook/plainCNN.jpg)\n","\n","Let's implement it! "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from model import PlainCNN\n","import torch\n","\n","# Testing the forward function e spiegare la formula per calcolare le dimensioni in una CNN\n","n_classes = 1\n","model = PlainCNN(config = [1,4,4,4,4], n_classes = n_classes)\n","model.test_forward(torch.randn(1,3,224,224))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from train_utils import train\n","from torch.optim import SGD\n","from torch.nn import BCEWithLogitsLoss\n","import torchvision.transforms as transf\n","from dataset import get_datasets, get_loaders\n","\n","# Define the transformations \n","transforms_train = transf.Compose([\n","                            transf.Resize((224,224), antialias=True), \n","                            transf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","                            ])\n","transforms_test_val = transf.Compose([\n","                            transf.Resize((224,224), antialias=True), \n","                            transf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","                            ])\n","\n","# Define the datasets and dataloaders\n","trainset, valset, testset = get_datasets(\"train\", \"test\", 0.1, 0.5, transforms_train, transforms_test_val)\n","trainloader, valloader, testloader = get_loaders(trainset, valset, testset, 16, num_workers=8, pin_memory=True)\n","\n","# Define the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define the model \n","model = PlainCNN([1,4,4,4,4], n_classes=1)\n","model.to(device)\n","print(\"Tot model parameters:\", sum([param.numel() for param in model.parameters()]))\n","print(\"Tot trainable parameters:\", sum([param.numel() for param in model.parameters() if param.requires_grad]))\n","\n","# Define the hyperparameters \n","learning_rate = 5e-3\n","epochs = 1\n","\n","# Define the optimizer\n","optimizer = SGD(model.parameters(),lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n","\n","# Define the loss function \n","loss_fn = BCEWithLogitsLoss(reduction=\"mean\")\n","\n","# Call the train function\n","train(model=model, \n","      trainloader=trainloader, \n","      valloader=valloader, \n","      epochs=epochs, \n","      optimizer=optimizer,\n","      loss_fn=loss_fn,\n","      device=device,\n","      save_directory=model._getname_())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","\n","# Explore the training and validation curves \n","with open(os.path.join(model._getname_(),\"acc_train.pkl\"),\"rb\") as f:\n","  acc_train = pickle.load(f)\n","with open(os.path.join(model._getname_(),\"loss_train.pkl\"),\"rb\") as f:\n","  loss_train = pickle.load(f)\n","with open(os.path.join(model._getname_(),\"acc_val.pkl\"),\"rb\") as f:\n","  acc_val = pickle.load(f)\n","with open(os.path.join(model._getname_(),\"loss_val.pkl\"),\"rb\") as f:\n","  loss_val = pickle.load(f)\n","  \n","plt.figure(figsize=(10,5))\n","plt.subplot(1,2,1)\n","plt.plot(acc_train, label=\"Training accuracy\")\n","plt.plot(acc_val, label=\"Validation accuracy\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.subplot(1,2,2)\n","plt.plot(loss_train, label=\"Training loss\")\n","plt.plot(loss_val, label=\"Validation loss\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### TEST THE MODEL\n","Now that we trained our model, we can test on some images from the test set. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchvision.io import read_image, ImageReadMode\n","\n","# Load the model \n","model = PlainCNN([1,4,4,4,4], n_classes=1)\n","model.load_state_dict(torch.load(os.path.join(model._getname_(),\"model_1_epochs.pt\")))\n","model.to(device)\n","\n","# Explore the data \n","files_test = os.listdir(\"test\")\n","random.seed(0)\n","random.shuffle(files_test)\n","fig = plt.figure(figsize=(15,15))\n","num_images = 16\n","transforms_test = transf.Compose([\n","                            transf.Resize((224,224)), \n","                            transf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","                            ])\n","model.eval()\n","with torch.no_grad():\n","  for num, fn in enumerate(files_test[:num_images]):\n","    path = os.path.join(\"test\", fn)\n","    img = read_image(path, mode=ImageReadMode.RGB) / 255\n","    out = model(transforms_test(img).unsqueeze(0).to(device))\n","    out = torch.sigmoid(out.squeeze())\n","    prediction = torch.round(out)\n","    plt.subplot(num_images//4,4,num+1)\n","    plt.axis('off')\n","    img = img.permute(1,2,0)\n","    plt.imshow(img)\n","    if(prediction):\n","      plt.title(\"This is a dog!\\n\"+str(round(out.item(),2)))\n","    else:\n","      plt.title(\"This is a cat!\\n\"+str(round(out.item(),2)))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# CODE PART 1: BUILDING AND TRAINING SKIP-CNN\n","\n","In this part, we will build and train a \"resnet\" convolutional neural network **with** skip-connections, again following the architecture described in the paper by He et al.  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from model import SkipCNN\n","import torch\n","\n","# Testing the forward function e spiegare la formula per calcolare le dimensioni in una CNN\n","n_classes = 1\n","model = SkipCNN(config=[1,2,2,2,2], n_classes=1)\n","model.test_forward(torch.randn(1,3,224,224))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from train_utils import train\n","from torch.optim import SGD\n","from torch.nn import BCEWithLogitsLoss\n","import torchvision.transforms as transf\n","from dataset import get_datasets, get_loaders\n","\n","# Define the transformations \n","transforms_train = transf.Compose([\n","                            transf.Resize((224,224), antialias=True), \n","                            transf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","                            ])\n","transforms_test_val = transf.Compose([\n","                            transf.Resize((224,224), antialias=True), \n","                            transf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","                            ])\n","\n","# Define the datasets and dataloaders\n","trainset, valset, testset = get_datasets(\"train\", \"test\", 0.1, 0.5, transforms_train, transforms_test_val)\n","trainloader, valloader, testloader = get_loaders(trainset, valset, testset, 16, num_workers=8, pin_memory=True)\n","\n","# Define the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define the model \n","model = SkipCNN([1,2,2,2,2], n_classes=1)\n","model.to(device)\n","print(\"Tot model parameters:\", sum([param.numel() for param in model.parameters()]))\n","print(\"Tot trainable parameters:\", sum([param.numel() for param in model.parameters() if param.requires_grad]))\n","\n","# Define the hyperparameters \n","learning_rate = 5e-3\n","epochs = 1\n","\n","# Define the optimizer\n","optimizer = SGD(model.parameters(),lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n","\n","# Define the loss function \n","loss_fn = BCEWithLogitsLoss(reduction=\"mean\")\n","\n","# Call the train function\n","train(model=model, \n","      trainloader=trainloader, \n","      valloader=valloader, \n","      epochs=epochs, \n","      optimizer=optimizer,\n","      loss_fn=loss_fn,\n","      device=device,\n","      save_directory=model._getname_())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Explore the training and validation curves \n","with open(os.path.join(model._getname_(),\"acc_train.pkl\"),\"rb\") as f:\n","  acc_train = pickle.load(f)\n","with open(os.path.join(model._getname_(),\"loss_train.pkl\"),\"rb\") as f:\n","  loss_train = pickle.load(f)\n","with open(os.path.join(model._getname_(),\"acc_val.pkl\"),\"rb\") as f:\n","  acc_val = pickle.load(f)\n","with open(os.path.join(model._getname_(),\"loss_val.pkl\"),\"rb\") as f:\n","  loss_val = pickle.load(f)\n","\n","  \n","plt.figure(figsize=(10,5))\n","plt.subplot(1,2,1)\n","plt.plot(acc_train, label=\"Training accuracy\")\n","plt.plot(acc_val, label=\"Validation accuracy\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.subplot(1,2,2)\n","plt.plot(loss_train, label=\"Training loss\")\n","plt.plot(loss_val, label=\"Validation loss\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open(os.path.join(\"PlainCNN_18_layers\",\"acc_train.pkl\"),\"rb\") as f:\n","  acc_train_plain = pickle.load(f)\n","with open(os.path.join(\"PlainCNN_18_layers\",\"loss_train.pkl\"),\"rb\") as f:\n","  loss_train_plain = pickle.load(f)\n","with open(os.path.join(\"PlainCNN_18_layers\",\"acc_val.pkl\"),\"rb\") as f:\n","  acc_val_plain = pickle.load(f)\n","with open(os.path.join(\"PlainCNN_18_layers\",\"loss_val.pkl\"),\"rb\") as f:\n","  loss_val_plain = pickle.load(f)\n","\n","plt.figure(figsize=(10,5))\n","plt.subplot(1,2,1)\n","plt.plot(acc_train, label=\"SKIP CNN\")\n","plt.plot(acc_train_plain, label=\"PLAIN CNN\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.subplot(1,2,2)\n","plt.plot(loss_train, label=\"SKIP CNN\")\n","plt.plot(loss_train_plain, label=\"PLAIN CNN\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchvision.io import read_image, ImageReadMode\n","\n","# Load the model \n","model = SkipCNN([1,2,2,2,2], n_classes=1)\n","model.load_state_dict(torch.load(os.path.join(model._getname_(),\"model_1_epochs.pt\")))\n","model.to(device)\n","\n","# Explore the data \n","files_test = os.listdir(\"test\")\n","random.seed(0)\n","random.shuffle(files_test)\n","fig = plt.figure(figsize=(15,15))\n","num_images = 16\n","transforms_test = transf.Compose([\n","                            transf.Resize((224,224)), \n","                            transf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","                            ])\n","model.eval()\n","with torch.no_grad():\n","  for num, fn in enumerate(files_test[:num_images]):\n","    path = os.path.join(\"test\", fn)\n","    img = read_image(path, mode=ImageReadMode.RGB) / 255\n","    out = model(transforms_test(img).unsqueeze(0).to(device))\n","    out = torch.sigmoid(out.squeeze())\n","    prediction = torch.round(out)\n","    plt.subplot(num_images//4,4,num+1)\n","    plt.axis('off')\n","    img = img.permute(1,2,0)\n","    plt.imshow(img)\n","    if(prediction):\n","      plt.title(\"This is a dog!\\n\"+str(round(out.item(),2)))\n","    else:\n","      plt.title(\"This is a cat!\\n\"+str(round(out.item(),2)))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["From this picture, it seems that with this shallow architectures, the skip-connections are not really necessary. Let's try with a deeper architecture, and see if the skip-connections help."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from train_utils import train\n","\n","# Define the datasets and dataloaders\n","trainset, valset, testset = get_datasets(\"train\", \"test\", 0.1, 0.5, transforms_train, transforms_test_val)\n","trainloader, valloader, testloader = get_loaders(trainset, valset, testset, 16, num_workers=8, pin_memory=True)\n","\n","# Define the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define the model \n","model = PlainCNN([1,6,8,12,6], n_classes=1) # 34 layers in total!\n","model.to(device)\n","print(\"Tot model parameters:\", sum([param.numel() for param in model.parameters()]))\n","print(\"Tot trainable parameters:\", sum([param.numel() for param in model.parameters() if param.requires_grad]))\n","\n","# Define the hyperparameters \n","learning_rate = 5e-3\n","epochs = 1\n","\n","# Define the optimizer\n","optimizer = SGD(model.parameters(),lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n","\n","# Define the loss function \n","loss_fn = BCEWithLogitsLoss(reduction=\"mean\")\n","\n","# Call the train function\n","train(model=model, \n","      trainloader=trainloader, \n","      valloader=valloader, \n","      epochs=epochs, \n","      optimizer=optimizer,\n","      loss_fn=loss_fn,\n","      device=device,\n","      save_directory=model._getname_())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Explore the training and validation curves \n","with open(os.path.join(model._getname_(),\"acc_train.pkl\"),\"rb\") as f:\n","  acc_train = pickle.load(f)\n","with open(os.path.join(model._getname_(),\"loss_train.pkl\"),\"rb\") as f:\n","  loss_train = pickle.load(f)\n","with open(os.path.join(model._getname_(),\"acc_val.pkl\"),\"rb\") as f:\n","  acc_val = pickle.load(f)\n","with open(os.path.join(model._getname_(),\"loss_val.pkl\"),\"rb\") as f:\n","  loss_val = pickle.load(f)\n","\n","\n","plt.figure(figsize=(10,5))\n","plt.subplot(1,2,1)\n","plt.plot(acc_train, label=\"Training accuracy\")\n","plt.plot(acc_val, label=\"Validation accuracy\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.subplot(1,2,2)\n","plt.plot(loss_train, label=\"Training loss\")\n","plt.plot(loss_val, label=\"Validation loss\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchvision.io import read_image, ImageReadMode\n","\n","# Load the model \n","model = PlainCNN([1,6,8,12,6], n_classes=1)\n","model.load_state_dict(torch.load(os.path.join(model._getname_(),\"model_1_epochs.pt\")))\n","model.to(device)\n","\n","# Explore the data \n","files_test = os.listdir(\"test\")\n","random.seed(0)\n","random.shuffle(files_test)\n","fig = plt.figure(figsize=(15,15))\n","num_images = 16\n","transforms_test = transf.Compose([\n","                            transf.Resize((224,224)), \n","                            transf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","                            ])\n","model.eval()\n","with torch.no_grad():\n","  for num, fn in enumerate(files_test[:num_images]):\n","    path = os.path.join(\"test\", fn)\n","    img = read_image(path, mode=ImageReadMode.RGB) / 255\n","    out = model(transforms_test(img).unsqueeze(0).to(device))\n","    out = torch.sigmoid(out.squeeze())\n","    prediction = torch.round(out)\n","    plt.subplot(num_images//4,4,num+1)\n","    plt.axis('off')\n","    img = img.permute(1,2,0)\n","    plt.imshow(img)\n","    if(prediction):\n","      plt.title(\"This is a dog!\\n\"+str(round(out.item(),2)))\n","    else:\n","      plt.title(\"This is a cat!\\n\"+str(round(out.item(),2)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from train_utils import train\n","\n","# Define the datasets and dataloaders\n","trainset, valset, testset = get_datasets(\"train\", \"test\", 0.1, 0.5, transforms_train, transforms_test_val)\n","trainloader, valloader, testloader = get_loaders(trainset, valset, testset, 16, num_workers=8, pin_memory=True)\n","\n","# Define the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define the model \n","model = SkipCNN([1,3,4,6,3], n_classes=1)\n","model.to(device)\n","print(\"Tot model parameters:\", sum([param.numel() for param in model.parameters()]))\n","print(\"Tot trainable parameters:\", sum([param.numel() for param in model.parameters() if param.requires_grad]))\n","\n","# Define the hyperparameters \n","learning_rate = 5e-3\n","epochs = 1\n","\n","# Define the optimizer\n","optimizer = SGD(model.parameters(),lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n","\n","# Define the loss function \n","loss_fn = BCEWithLogitsLoss(reduction=\"mean\")\n","\n","# Call the train function\n","train(model=model, \n","      trainloader=trainloader, \n","      valloader=valloader, \n","      epochs=epochs, \n","      optimizer=optimizer,\n","      loss_fn=loss_fn,\n","      device=device,\n","      save_directory=model._getname_())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Explore the training and validation curves \n","with open(os.path.join(model._getname_(),\"acc_train.pkl\"),\"rb\") as f:\n","  acc_train = pickle.load(f)\n","with open(os.path.join(model._getname_(),\"loss_train.pkl\"),\"rb\") as f:\n","  loss_train = pickle.load(f)\n","with open(os.path.join(model._getname_(),\"acc_val.pkl\"),\"rb\") as f:\n","  acc_val = pickle.load(f)\n","with open(os.path.join(model._getname_(),\"loss_val.pkl\"),\"rb\") as f:\n","  loss_val = pickle.load(f)\n","\n","\n","plt.figure(figsize=(10,5))\n","plt.subplot(1,2,1)\n","plt.plot(acc_train, label=\"Training accuracy\")\n","plt.plot(acc_val, label=\"Validation accuracy\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.subplot(1,2,2)\n","plt.plot(loss_train, label=\"Training loss\")\n","plt.plot(loss_val, label=\"Validation loss\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open(os.path.join(\"PlainCNN_18_layers\",\"acc_train.pkl\"),\"rb\") as f:\n","  acc_train_plain = pickle.load(f)\n","with open(os.path.join(\"PlainCNN_18_layers\",\"loss_train.pkl\"),\"rb\") as f:\n","  loss_train_plain = pickle.load(f)\n","with open(os.path.join(\"PlainCNN_18_layers\",\"acc_val.pkl\"),\"rb\") as f:\n","  acc_val_plain = pickle.load(f)\n","with open(os.path.join(\"PlainCNN_18_layers\",\"loss_val.pkl\"),\"rb\") as f:\n","  loss_val_plain = pickle.load(f)\n","\n","plt.figure(figsize=(10,5))\n","plt.subplot(1,2,1)\n","plt.plot(acc_train, label=\"SKIP CNN\")\n","plt.plot(acc_train_plain, label=\"PLAIN CNN\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.subplot(1,2,2)\n","plt.plot(loss_train, label=\"SKIP CNN\")\n","plt.plot(loss_train_plain, label=\"PLAIN CNN\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchvision.io import read_image, ImageReadMode\n","\n","# Load the model \n","model = SkipCNN([1,3,4,6,3], n_classes=1)\n","model.load_state_dict(torch.load(os.path.join(model._getname_(),\"model_1_epochs.pt\")))\n","model.to(device)\n","\n","# Explore the data \n","files_test = os.listdir(\"test\")\n","random.seed(0)\n","random.shuffle(files_test)\n","fig = plt.figure(figsize=(15,15))\n","num_images = 16\n","transforms_test = transf.Compose([\n","                            transf.Resize((224,224)), \n","                            transf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","                            ])\n","model.eval()\n","with torch.no_grad():\n","  for num, fn in enumerate(files_test[:num_images]):\n","    path = os.path.join(\"test\", fn)\n","    img = read_image(path, mode=ImageReadMode.RGB) / 255\n","    out = model(transforms_test(img).unsqueeze(0).to(device))\n","    out = torch.sigmoid(out.squeeze())\n","    prediction = torch.round(out)\n","    plt.subplot(num_images//4,4,num+1)\n","    plt.axis('off')\n","    img = img.permute(1,2,0)\n","    plt.imshow(img)\n","    if(prediction):\n","      plt.title(\"This is a dog!\\n\"+str(round(out.item(),2)))\n","    else:\n","      plt.title(\"This is a cat!\\n\"+str(round(out.item(),2)))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**CODE PART 7: TRANSFER LEARNING**\n","\n","In general, building a network from scratch and training it with limited data is not a good idea!\n","Transfer learning equip us with a strategy to leverage a pre-trained model, and adapt it to fit our task. Furthermore, pretrained networks are able to extract very general and powerful representations, which can be very useful when fine-tuned on a target dataset. \n","In general, we will try to a pretrained resnet18 model, which have been trained on the ImageNet dataset (a large image classification dataset), and then fine-tune it on our cats vs dogs dataset, to see if we can achieve a better accuracy."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Usually, transfer learning is first done in those steps:\n","- modify the last layer of the network to fit the number of classes of the target dataset. \n","- train the network by freezing all the layers in the network except the one modified.\n","- unfreeze the network, and train the whole network for some epochs with a lower learning rate.\n","\n","Let's see how straightforward it is to do this in PyTorch."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchvision.models import resnet18, ResNet18_Weights # Import the resnet architecture and pretrained weights from torchvision.models \n","\n","trainset, valset, testset = get_datasets(\"train\", \"test\", 0.1, 1, transforms_train, transforms_test_val)\n","trainloader, valloader, testloader = get_loaders(trainset, valset, testset, 16, 8, True)\n","\n","# Use cuda if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Get the model \n","model = resnet18(weights=ResNet18_Weights.DEFAULT) \n","# Replace the last layer to fit the output classes of the new task.\n","model.fc = nn.Linear(512,1)\n","# Freeze all layers except the last fully conneceted layer\n","for name, param in model.named_parameters():\n","  if 'fc' not in name:\n","    param.requires_grad = False\n","    \n","model.to(device)\n","\n","print(\"Tot model parameters:\", sum([param.numel() for param in model.parameters()]))\n","print(\"Tot trainable parameters:\", sum([param.numel() for param in model.parameters() if param.requires_grad]))\n","\n","# Hyperparameters \n","learning_rate = 1e-3 # With transfer learning we can use a lower learning rate\n","epochs = 10\n","\n","# Get the optimizer\n","optimizer = SGD(model.parameters(),lr=learning_rate, weight_decay=1e-4, momentum=0.9) \n","\n","# Get the loss function \n","loss_fn = BCEWithLogitsLoss(reduction=\"mean\")\n","\n","# Call the train function\n","train(model=model, \n","      trainloader=trainloader, \n","      valloader=valloader, \n","      epochs=epochs, \n","      optimizer=optimizer,\n","      loss_fn=loss_fn,\n","      device=device,\n","      save_directory=\"Resnet18_linear_probing\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Explore the data \n","files_test = os.listdir(\"test\")\n","random.seed(0)\n","random.shuffle(files_test)\n","fig = plt.figure(figsize=(15,15))\n","num_images = 16\n","transforms_test = transf.Compose([\n","                            transf.ToTensor(),\n","                            transf.Resize((224,224)), \n","                            transf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","                            ])\n","model.eval()\n","with torch.no_grad():\n","  for num, fn in enumerate(files_test[:num_images]):\n","    path = os.path.join(\"test\",fn)\n","    img = Image.open(path)\n","    out = model(transforms_test(img).unsqueeze(0).to(device))\n","    out = torch.sigmoid(out.squeeze())\n","    prediction = torch.round(out)\n","    plt.subplot(num_images//4,4,num+1)\n","    plt.axis('off')\n","    plt.imshow(img)\n","    if(prediction):\n","      plt.title(\"This is a dog!\\n\"+str(round(out.item(),2)))\n","    else:\n","      plt.title(\"This is a cat!\\n\"+str(round(out.item(),2)))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now unfreeze the complete network and train it for 5 epochs with a lower learning rate."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchvision.models import resnet18\n","\n","trainset, valset, testset = get_datasets(\"train\", \"test\", 0.1, 1, transforms_train, transforms_test_val) # Finetune on the complete dataset \n","trainloader, valloader, testloader = get_loaders(trainset, valset, testset, 16, 8, True)\n","\n","# Use cuda if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Get the model \n","model = resnet18() \n","# Replace the last layer of the model with a new one\n","model.fc = nn.Linear(512,1)\n","# Load the previous weights to continue training\n","model.load_state_dict(torch.load(\"weights/resnet18_linear_probing.pth\"))\n","\n","model.to(device)\n","\n","print(\"Tot model parameters:\", sum([param.numel() for param in model.parameters()]))\n","print(\"Tot trainable parameters:\", sum([param.numel() for param in model.parameters() if param.requires_grad]))\n","\n","# Hyperparameters \n","learning_rate = 1e-4 # DECREASE THE LEARNING RATE\n","epochs = 5\n","\n","# Get the optimizer\n","optimizer = SGD(model.parameters(),lr=learning_rate, weight_decay=1e-6, momentum=0.9) # DECREASE WEIGHT DECAY\n","\n","# Get the loss function \n","loss_fn = BCEWithLogitsLoss(reduction=\"mean\")\n","\n","# Call the train function\n","train(model=model, \n","      trainloader=trainloader, \n","      valloader=valloader, \n","      epochs=epochs, \n","      optimizer=optimizer,\n","      loss_fn=loss_fn,\n","      device=device,\n","      save_directory=\"Resnet18_finetuning\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Explore the data \n","files_test = os.listdir(\"test\")\n","random.seed(0)\n","random.shuffle(files_test)\n","fig = plt.figure(figsize=(15,15))\n","num_images = 16\n","transforms_test = transf.Compose([\n","                            transf.ToTensor(),\n","                            transf.Resize((224,224)), \n","                            transf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","                            ])\n","model.eval()\n","with torch.no_grad():\n","  for num, fn in enumerate(files_test[:num_images]):\n","    path = os.path.join(\"test\",fn)\n","    img = Image.open(path)\n","    out = model(transforms_test(img).unsqueeze(0).to(device))\n","    out = torch.sigmoid(out.squeeze())\n","    prediction = torch.round(out)\n","    plt.subplot(num_images//4,4,num+1)\n","    plt.axis('off')\n","    plt.imshow(img)\n","    if(prediction):\n","      plt.title(\"This is a dog!\\n\"+str(round(out.item(),2)))\n","    else:\n","      plt.title(\"This is a cat!\\n\"+str(round(out.item(),2)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save the weights of the networks\n","torch.save(model.state_dict(), \"weights/resnet18_finetuning.pth\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# LET'S TRY TO PREDICT MORE IMAGES WITH OUR MOST POWERFUL MODEL! "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Explore the data \n","files_test = os.listdir(\"test\")\n","random.seed(0)\n","random.shuffle(files_test)\n","fig = plt.figure(figsize=(15,80))\n","num_images = 128\n","transforms_test = transf.Compose([\n","                            transf.ToTensor(),\n","                            transf.Resize((224,224)), \n","                            transf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","                            ])\n","model.eval()\n","with torch.no_grad():\n","  for num, fn in enumerate(files_test[:num_images]):\n","    path = os.path.join(\"test\",fn)\n","    img = Image.open(path)\n","    out = model(transforms_test(img).unsqueeze(0).to(device))\n","    prediction = out.squeeze()>0.5\n","    plt.subplot(num_images//4,4,num+1)\n","    plt.axis('off')\n","    plt.imshow(img)\n","    if(prediction):\n","      plt.title(\"This is a dog!\")\n","    else:\n","      plt.title(\"This is a cat!\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let's try to submit the solution to kaggle!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Submission must be in the format of a csv file with two columns: id and label\n","import csv\n","\n","files_test = os.listdir(\"test\")\n","\n","with open('submission_kaggle.csv', 'w', newline='') as file:\n","  writer = csv.writer(file)\n","\n","  writer.writerow([\"id\", \"label\"])\n","  model.eval()\n","  with torch.no_grad():\n","    for num, fn in enumerate(tqdm(files_test)): # ALL IMAGES\n","      id_img = fn.split(\".\")[0]\n","      path = os.path.join(\"test\",fn)\n","      img = Image.open(path)\n","      out = model(transforms_test(img).unsqueeze(0).to(device))\n","      prediction = torch.clamp(out.squeeze(),min=0.02, max=0.98).item()\n","      writer.writerow([str(id_img),str(prediction)])\n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We would have ended up in 196 th position out of more or less 1350 people!"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMBMOdwLn83pc4CCsHogsnV","mount_file_id":"18d5LXhD-HFqZNbPwpVHP7Lwh6PzFPS3p","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"testCUDA","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"vscode":{"interpreter":{"hash":"1d671a04252fa3c445ea7448c3b2c3b54821da269c15286604d3f895b900c469"}}},"nbformat":4,"nbformat_minor":0}
