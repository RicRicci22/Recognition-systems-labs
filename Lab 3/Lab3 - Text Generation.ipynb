{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["with open(\"gameofthrones.txt\", \"r\") as file:\n","    text = file.read()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Lenght of the book in characters:  5662324\n"]}],"source":["print(\"Lenght of the book in characters: \", len(text))"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","“We should start back,” Gared urged as the woods began to grow dark around them. “The wildlings are dead.”\n","\n","“Do the dead frighten you?” Ser Waymar Royce asked with just the hint of a smile.\n","\n","Gared did not rise to the bait. He was an old man, past fifty, and he had seen the lordlings come and go. “Dead is dead,” he said. “We have no business with the dead.”\n","\n","“Are they dead?” Royce asked softly. “What proof have we?”\n","\n","“Will saw them,” Gared said. “If he says they are dead, that’s proof enough for me.”\n","\n","Will had known they would drag him into the quarrel sooner or later. He wished it had been later rather than sooner. “My mother told me that dead men sing no songs,” he put in.\n","\n","“My wet nurse said the same thing, Will,” Royce replied. “Never believe anything you hear at a woman’s tit. There are things to be learned even from the dead.” His voice echoed, too loud in the twilit forest.\n","\n","“We have a long ride before us,” Gared pointed out. “Eight days, maybe nine. And night is falling.”\n","\n","Ser\n"]}],"source":["print(text[:1000])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Let's try to explore a character based encoding"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["'\\n !(),-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyz{}éê—‘’“”…'\n","Number of unique characters:  86\n"]}],"source":["chars = sorted(list(set(text)))\n","print(repr(\"\".join(chars)))\n","print(\"Number of unique characters: \", len(chars))"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Create a mapping from characters to indices (vocabulary)\n","char_to_idx = {ch:i for i, ch in enumerate(chars)}\n","idx_to_char = {i:ch for i, ch in enumerate(chars)}\n","\n","encode_char = lambda s: [char_to_idx[c] for c in s] # s is the input string that I want to encode\n","decode_char = lambda l: \"\".join([idx_to_char[i] for i in l]) # l is the input list of indices that I want to decode\n","show_single_tokens = lambda s: [c for c in s]"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Decoded tokens:  ['T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', ' ', 't', 'e', 's', 't', '!']\n","Encoded string:  [41, 57, 58, 68, 1, 58, 68, 1, 50, 1, 69, 54, 68, 69, 2]\n","Decoded string:  This is a test!\n"]}],"source":["# Try to encode and decode a string\n","string = \"This is a test!\"\n","decoded_tokens = show_single_tokens(string)\n","print(\"Decoded tokens: \", decoded_tokens)\n","encoded = encode_char(string)\n","print(\"Encoded string: \", encoded)\n","decoded = decode_char(encoded)\n","print(\"Decoded string: \", decoded)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Let's try to explore a word based encoding"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['!', '(', ')', ',', '.', '..', '0', '1-37', '103-129', '10th', '129-131', '131-157', '157-161', '15th', '161-171', '16th', '171-172', '172-184', '184-209', '209-221', '221-233', '233-259', '23rd', '259-262', '262-283', '37-42', '42-48', '48-103', '57th', '61st', ':', ';', '?', 'A', 'AAHOOOOOOO', 'AAHOOOOOOOOOOOO.', 'ACROSS', 'ADDAM', 'AEGON', 'AEMON', 'AENYS', 'AERYS', 'AETHAN', 'AIR', 'ALADALE', 'ALAN', 'ALANNYS', 'ALARIC', 'ALAYAYA', 'ALBAR', 'ALBETT', 'ALEBELLY', 'ALERIE', 'ALESANDER', 'ALESTER', 'ALESTIR', 'ALFYN', 'ALL', 'ALLA', 'ALLARD', 'ALLISER', 'ALLYRION', 'ALYCE', 'ALYN', 'ALYSANNE', 'ALYSSA', 'ALYX', 'AMAREI', 'AMBROSE', 'AMEREI', 'AMI', 'AMORY', 'AN', 'AND', 'ANDAR', 'ANDREW', 'ANDROW', 'ANDROX', 'ANNARA', 'ANTARIO', 'ANYA', 'APPENDIX', 'APPLE-EATER', 'ARCHER', 'ARE', 'ARIANNE', 'ARON', 'ARRON', 'ARRY', 'ARRYK', 'ARRYN', 'ARSTAN', 'ARTHUR', 'ARWOOD', 'ARYA', 'ARYS', 'ASSHAI', 'AXELL', 'Aaaaahoooooooooooooooooooooooooo', 'Abandoned…']\n","Number of words:  29049\n"]}],"source":["import nltk\n","from nltk.tokenize import word_tokenize # pip install nltk\n","\n","# Get the tokenizer\n","#nltk.download('punkt')\n","\n","words = sorted(list(set(word_tokenize(text))))\n","print(words[:100])\n","print(\"Number of words: \", len(words))"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[],"source":["# Create a mapping from characters to indices (vocabulary)\n","word_to_idx = {word:i for i, word in enumerate(words)}\n","idx_to_word = {i:word for i, word in enumerate(words)}\n","\n","encode_word = lambda s: [word_to_idx[word] for word in word_tokenize(s)] # s is the input string that I want to encode\n","decode_word = lambda l: \" \".join([idx_to_word[i] for i in l]) # l is the input list of indices that I want to decode\n","show_single_tokens = lambda s: word_tokenize(s)"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Decoded tokens:  ['This', 'is', 'a', 'test', '!']\n","Encoded string:  [5950, 16620, 6811, 25679, 0]\n","Decoded string:  This is a test !\n"]}],"source":["# Try to encode and decode a string\n","decoded_tokens = show_single_tokens(string)\n","print(\"Decoded tokens: \", decoded_tokens)\n","string = \"This is a test!\"\n","encoded = encode_word(string)\n","print(\"Encoded string: \", encoded)\n","decoded = decode_word(encoded)\n","print(\"Decoded string: \", decoded)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Let's try to explore a sub word based tokenizer (the one used in chatGPT!)"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[],"source":["import tiktoken\n","\n","enc = tiktoken.get_encoding(\"cl100k_base\")\n","\n","encode_subword = lambda s: enc.encode(s) # s is the input string that I want to encode\n","decode_subword = lambda l: enc.decode(l) # l is the input list of indices that I want to decode\n","show_single_tokens = lambda l: [enc.decode_single_token_bytes(i) for i in l]"]},{"cell_type":"code","execution_count":136,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoded string:  [2028, 374, 264, 1296, 0]\n","Decoded tokens:  [b'This', b' is', b' a', b' test', b'!']\n","Decoded string:  This is a test!\n"]}],"source":["# Try to encode and decode a string\n","string = \"This is a test!\"\n","encoded = encode_subword(string)\n","print(\"Encoded string: \", encoded)\n","decoded_tokens = show_single_tokens(encoded)\n","print(\"Decoded tokens: \", decoded_tokens)\n","decoded = decode_subword(encoded)\n","print(\"Decoded string: \", decoded)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## For simplicity, we will implement our networks using a character based tokenization"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([5662324])\n","tensor([ 0,  0, 83, 44, 54,  1, 68, 57, 64, 70, 61, 53,  1, 68, 69, 50, 67, 69,\n","         1, 51, 50, 52, 60,  5, 84,  1, 28, 50, 67, 54, 53,  1, 70, 67, 56, 54,\n","        53,  1, 50, 68,  1, 69, 57, 54,  1, 72, 64, 64, 53, 68,  1, 51, 54, 56,\n","        50, 63,  1, 69, 64,  1, 56, 67, 64, 72,  1, 53, 50, 67, 60,  1, 50, 67,\n","        64, 70, 63, 53,  1, 69, 57, 54, 62,  7,  1, 83, 41, 57, 54,  1, 72, 58,\n","        61, 53, 61, 58, 63, 56, 68,  1, 50, 67])\n"]}],"source":["import torch \n","data = torch.tensor(encode_char(text), dtype=torch.long)\n","print(data.shape)\n","print(data[:100])"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Split the dataset in training and validation\n","# I want to store 90% of characters for training and 10% for validation\n","n = int(0.9*len(text))\n","train_data = data[:n]\n","val_data = data[n:]"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import random\n","\n","context_length = 8 # This is maximum number of tokens that are allowed to fit in the context. \n","\n","def get_batch(data, batch_size = 8, context_length = 8):\n","    # Get batch_size random indices in the data\n","    random_idx = random.sample(range(len(data)-context_length), batch_size)\n","    # Pluck the next character after each random index\n","    inputs = torch.zeros((batch_size, context_length), dtype=torch.long)\n","    targets = torch.zeros((batch_size, context_length), dtype=torch.long)\n","    \n","    for i in range(batch_size):\n","        inputs[i,:] = data[random_idx[i]:random_idx[i]+context_length]\n","        targets[i,:] = data[random_idx[i]+1:random_idx[i]+context_length+1]\n","    \n","    return inputs, targets"]},{"cell_type":"code","execution_count":141,"metadata":{},"outputs":[],"source":["# Let's build a bigram language model \n","import torch.nn as nn \n","import torch.nn.functional as F\n","\n","class BigramLM(nn.Module):\n","    def __init__(self, vocab_size):\n","        super(BigramLM, self).__init__()\n","        self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n","    \n","    def forward(self, inputs):\n","        # Receive inputs in the form (B, L, C) (batch times seq length times num classes (tokens))\n","        # Embed the inputs\n","        embeddings = self.embedding_table(inputs)\n","        return embeddings\n","    \n","    def generate(self, inputs, max_new_tokens=10):\n","        # Generate next tokens given the inputs (B, L, C)\n","        for i in range(max_new_tokens):                   \n","            # Embed the inputs\n","            logits = self(inputs)\n","            # Get the last logit \n","            last_logit = logits[:, -1, :] # Get the last element in the length dimension\n","            # Compute the probabilities\n","            probs = F.softmax(last_logit, dim=1)\n","            # Get the next token\n","            next_token = torch.multinomial(probs, num_samples=1)\n","            # Append to the inputs\n","            inputs = torch.cat((inputs, next_token), dim=1)\n","            \n","        return inputs"]},{"cell_type":"code","execution_count":146,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","]WI?DZ9]jbMK3AyvQd7…Y0pY;h{If{’bCu2Q{vX(;]W}X9.mN”cE“--oY(Mc‘wQBa]0\n","Hyy 9(’cNP6f51LWyORG,be!’’y/!…ê}\n"]}],"source":["model = BigramLM(len(chars))\n","\n","inputs_generate = torch.zeros((1,1), dtype=torch.long)\n","print(decode_char(model.generate(inputs_generate, max_new_tokens=100)[0].tolist()))"]},{"cell_type":"code","execution_count":147,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  1%|▏         | 1257/100000 [00:02<01:19, 1248.55it/s]"]},{"name":"stdout","output_type":"stream","text":["4.687668268203735\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 2194/100000 [00:02<01:06, 1473.90it/s]"]},{"name":"stdout","output_type":"stream","text":["4.079787847280502\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 3156/100000 [00:03<01:00, 1594.11it/s]"]},{"name":"stdout","output_type":"stream","text":["3.696454896211624\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 4250/100000 [00:04<01:01, 1547.33it/s]"]},{"name":"stdout","output_type":"stream","text":["3.4849761803150177\n"]},{"name":"stderr","output_type":"stream","text":["  5%|▌         | 5174/100000 [00:04<01:05, 1453.49it/s]"]},{"name":"stdout","output_type":"stream","text":["3.323266463518143\n"]},{"name":"stderr","output_type":"stream","text":["  6%|▌         | 6136/100000 [00:05<00:59, 1565.22it/s]"]},{"name":"stdout","output_type":"stream","text":["3.2166521780490873\n"]},{"name":"stderr","output_type":"stream","text":["  7%|▋         | 7214/100000 [00:06<01:01, 1520.25it/s]"]},{"name":"stdout","output_type":"stream","text":["3.1230453526973725\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 8161/100000 [00:06<01:01, 1504.38it/s]"]},{"name":"stdout","output_type":"stream","text":["3.0583948345184324\n"]},{"name":"stderr","output_type":"stream","text":["  9%|▉         | 9254/100000 [00:07<01:00, 1511.86it/s]"]},{"name":"stdout","output_type":"stream","text":["2.978463546037674\n"]},{"name":"stderr","output_type":"stream","text":[" 10%|█         | 10184/100000 [00:08<00:57, 1555.80it/s]"]},{"name":"stdout","output_type":"stream","text":["2.9407554030418397\n"]},{"name":"stderr","output_type":"stream","text":[" 11%|█         | 11123/100000 [00:08<00:59, 1496.26it/s]"]},{"name":"stdout","output_type":"stream","text":["2.9024832456111906\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 12225/100000 [00:09<00:56, 1545.52it/s]"]},{"name":"stdout","output_type":"stream","text":["2.8745863375663756\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 13134/100000 [00:10<00:58, 1478.31it/s]"]},{"name":"stdout","output_type":"stream","text":["2.832597141265869\n"]},{"name":"stderr","output_type":"stream","text":[" 14%|█▍        | 14225/100000 [00:11<00:57, 1484.50it/s]"]},{"name":"stdout","output_type":"stream","text":["2.801151771783829\n"]},{"name":"stderr","output_type":"stream","text":[" 15%|█▌        | 15178/100000 [00:11<00:55, 1529.73it/s]"]},{"name":"stdout","output_type":"stream","text":["2.79246559715271\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|█▋        | 16289/100000 [00:12<00:53, 1572.51it/s]"]},{"name":"stdout","output_type":"stream","text":["2.7742701733112334\n"]},{"name":"stderr","output_type":"stream","text":[" 17%|█▋        | 17227/100000 [00:13<00:55, 1480.82it/s]"]},{"name":"stdout","output_type":"stream","text":["2.7545707128047945\n"]},{"name":"stderr","output_type":"stream","text":[" 18%|█▊        | 18168/100000 [00:13<00:53, 1534.16it/s]"]},{"name":"stdout","output_type":"stream","text":["2.7225119044780732\n"]},{"name":"stderr","output_type":"stream","text":[" 19%|█▉        | 19254/100000 [00:14<00:53, 1504.04it/s]"]},{"name":"stdout","output_type":"stream","text":["2.7201771183013914\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|██        | 20301/100000 [00:15<00:54, 1454.44it/s]"]},{"name":"stdout","output_type":"stream","text":["2.696537539720535\n"]},{"name":"stderr","output_type":"stream","text":[" 21%|██        | 21232/100000 [00:15<00:54, 1444.69it/s]"]},{"name":"stdout","output_type":"stream","text":["2.690617923736572\n"]},{"name":"stderr","output_type":"stream","text":[" 22%|██▏       | 22274/100000 [00:16<00:51, 1514.57it/s]"]},{"name":"stdout","output_type":"stream","text":["2.668888926267624\n"]},{"name":"stderr","output_type":"stream","text":[" 23%|██▎       | 23150/100000 [00:17<00:54, 1405.22it/s]"]},{"name":"stdout","output_type":"stream","text":["2.6627286520004274\n"]},{"name":"stderr","output_type":"stream","text":[" 24%|██▍       | 24240/100000 [00:17<00:59, 1282.39it/s]"]},{"name":"stdout","output_type":"stream","text":["2.6532003712654113\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▌       | 25281/100000 [00:18<00:53, 1406.13it/s]"]},{"name":"stdout","output_type":"stream","text":["2.660126284599304\n"]},{"name":"stderr","output_type":"stream","text":[" 26%|██▌       | 26169/100000 [00:19<00:49, 1480.47it/s]"]},{"name":"stdout","output_type":"stream","text":["2.632172062635422\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 27195/100000 [00:20<01:00, 1203.72it/s]"]},{"name":"stdout","output_type":"stream","text":["2.629861470222473\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 28254/100000 [00:20<00:50, 1420.12it/s]"]},{"name":"stdout","output_type":"stream","text":["2.6192698934078216\n"]},{"name":"stderr","output_type":"stream","text":[" 29%|██▉       | 29191/100000 [00:21<00:46, 1510.50it/s]"]},{"name":"stdout","output_type":"stream","text":["2.6154126760959624\n"]},{"name":"stderr","output_type":"stream","text":[" 30%|███       | 30266/100000 [00:22<00:45, 1535.70it/s]"]},{"name":"stdout","output_type":"stream","text":["2.6067468729019163\n"]},{"name":"stderr","output_type":"stream","text":[" 31%|███       | 31176/100000 [00:23<00:51, 1324.10it/s]"]},{"name":"stdout","output_type":"stream","text":["2.6095493750572203\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 32282/100000 [00:23<00:43, 1544.60it/s]"]},{"name":"stdout","output_type":"stream","text":["2.5926264505386354\n"]},{"name":"stderr","output_type":"stream","text":[" 33%|███▎      | 33222/100000 [00:24<00:42, 1574.44it/s]"]},{"name":"stdout","output_type":"stream","text":["2.5844400000572203\n"]},{"name":"stderr","output_type":"stream","text":[" 34%|███▍      | 34149/100000 [00:25<00:44, 1491.13it/s]"]},{"name":"stdout","output_type":"stream","text":["2.586838266611099\n"]},{"name":"stderr","output_type":"stream","text":[" 35%|███▌      | 35251/100000 [00:25<00:41, 1550.36it/s]"]},{"name":"stdout","output_type":"stream","text":["2.5773459968566894\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▋      | 36351/100000 [00:26<00:40, 1563.73it/s]"]},{"name":"stdout","output_type":"stream","text":["2.581869584321976\n"]},{"name":"stderr","output_type":"stream","text":[" 37%|███▋      | 37273/100000 [00:27<00:42, 1487.20it/s]"]},{"name":"stdout","output_type":"stream","text":["2.5790938668251036\n"]},{"name":"stderr","output_type":"stream","text":[" 38%|███▊      | 38213/100000 [00:27<00:41, 1497.19it/s]"]},{"name":"stdout","output_type":"stream","text":["2.57426514172554\n"]},{"name":"stderr","output_type":"stream","text":[" 39%|███▉      | 39252/100000 [00:28<00:44, 1367.79it/s]"]},{"name":"stdout","output_type":"stream","text":["2.564671741962433\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|████      | 40309/100000 [00:29<00:39, 1506.28it/s]"]},{"name":"stdout","output_type":"stream","text":["2.551949036836624\n"]},{"name":"stderr","output_type":"stream","text":[" 41%|████▏     | 41251/100000 [00:29<00:38, 1529.79it/s]"]},{"name":"stdout","output_type":"stream","text":["2.5536060841083525\n"]},{"name":"stderr","output_type":"stream","text":[" 42%|████▏     | 42200/100000 [00:30<00:36, 1566.32it/s]"]},{"name":"stdout","output_type":"stream","text":["2.551842971086502\n"]},{"name":"stderr","output_type":"stream","text":[" 43%|████▎     | 43305/100000 [00:31<00:37, 1496.01it/s]"]},{"name":"stdout","output_type":"stream","text":["2.556311635732651\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▍     | 44231/100000 [00:31<00:37, 1487.21it/s]"]},{"name":"stdout","output_type":"stream","text":["2.5490309101343156\n"]},{"name":"stderr","output_type":"stream","text":[" 45%|████▌     | 45193/100000 [00:32<00:34, 1602.43it/s]"]},{"name":"stdout","output_type":"stream","text":["2.5489499163627625\n"]},{"name":"stderr","output_type":"stream","text":[" 46%|████▋     | 46293/100000 [00:33<00:34, 1535.63it/s]"]},{"name":"stdout","output_type":"stream","text":["2.550132618069649\n"]},{"name":"stderr","output_type":"stream","text":[" 47%|████▋     | 47219/100000 [00:33<00:36, 1465.34it/s]"]},{"name":"stdout","output_type":"stream","text":["2.540791068792343\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 48154/100000 [00:34<00:33, 1553.87it/s]"]},{"name":"stdout","output_type":"stream","text":["2.5376227912902833\n"]},{"name":"stderr","output_type":"stream","text":[" 49%|████▉     | 49225/100000 [00:35<00:34, 1483.84it/s]"]},{"name":"stdout","output_type":"stream","text":["2.5265587198734285\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50188/100000 [00:35<00:33, 1497.66it/s]"]},{"name":"stdout","output_type":"stream","text":["2.534102538108826\n"]},{"name":"stderr","output_type":"stream","text":[" 51%|█████▏    | 51299/100000 [00:36<00:30, 1595.72it/s]"]},{"name":"stdout","output_type":"stream","text":["2.5279804809093473\n"]},{"name":"stderr","output_type":"stream","text":[" 52%|█████▏    | 52218/100000 [00:37<00:31, 1509.01it/s]"]},{"name":"stdout","output_type":"stream","text":["2.5259098660945893\n"]},{"name":"stderr","output_type":"stream","text":[" 53%|█████▎    | 53145/100000 [00:37<00:31, 1487.86it/s]"]},{"name":"stdout","output_type":"stream","text":["2.5228976025581358\n"]},{"name":"stderr","output_type":"stream","text":[" 54%|█████▍    | 54264/100000 [00:38<00:28, 1602.14it/s]"]},{"name":"stdout","output_type":"stream","text":["2.516157965898514\n"]},{"name":"stderr","output_type":"stream","text":[" 55%|█████▌    | 55338/100000 [00:39<00:29, 1494.75it/s]"]},{"name":"stdout","output_type":"stream","text":["2.520866632938385\n"]},{"name":"stderr","output_type":"stream","text":[" 56%|█████▌    | 56244/100000 [00:40<00:30, 1450.61it/s]"]},{"name":"stdout","output_type":"stream","text":["2.52144296836853\n"]},{"name":"stderr","output_type":"stream","text":[" 57%|█████▋    | 57192/100000 [00:40<00:28, 1516.26it/s]"]},{"name":"stdout","output_type":"stream","text":["2.5097784202098845\n"]},{"name":"stderr","output_type":"stream","text":[" 58%|█████▊    | 58265/100000 [00:41<00:27, 1512.56it/s]"]},{"name":"stdout","output_type":"stream","text":["2.525925309896469\n"]},{"name":"stderr","output_type":"stream","text":[" 59%|█████▉    | 59176/100000 [00:42<00:30, 1334.54it/s]"]},{"name":"stdout","output_type":"stream","text":["2.5117537341117857\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|██████    | 60247/100000 [00:42<00:27, 1465.38it/s]"]},{"name":"stdout","output_type":"stream","text":["2.516458915948868\n"]},{"name":"stderr","output_type":"stream","text":[" 61%|██████    | 61175/100000 [00:43<00:25, 1534.93it/s]"]},{"name":"stdout","output_type":"stream","text":["2.505888326048851\n"]},{"name":"stderr","output_type":"stream","text":[" 62%|██████▏   | 62225/100000 [00:44<00:26, 1449.51it/s]"]},{"name":"stdout","output_type":"stream","text":["2.512193487405777\n"]},{"name":"stderr","output_type":"stream","text":[" 63%|██████▎   | 63151/100000 [00:44<00:27, 1352.91it/s]"]},{"name":"stdout","output_type":"stream","text":["2.5067840993404387\n"]},{"name":"stderr","output_type":"stream","text":[" 64%|██████▍   | 64285/100000 [00:45<00:25, 1413.30it/s]"]},{"name":"stdout","output_type":"stream","text":["2.5059978873729705\n"]},{"name":"stderr","output_type":"stream","text":[" 65%|██████▌   | 65275/100000 [00:46<00:24, 1413.55it/s]"]},{"name":"stdout","output_type":"stream","text":["2.5018182773590087\n"]},{"name":"stderr","output_type":"stream","text":[" 66%|██████▋   | 66294/100000 [00:47<00:24, 1401.43it/s]"]},{"name":"stdout","output_type":"stream","text":["2.5109407860040664\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 67186/100000 [00:48<00:28, 1143.99it/s]"]},{"name":"stdout","output_type":"stream","text":["2.5006506083011626\n"]},{"name":"stderr","output_type":"stream","text":[" 68%|██████▊   | 68226/100000 [00:48<00:24, 1276.04it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4997402098178862\n"]},{"name":"stderr","output_type":"stream","text":[" 69%|██████▉   | 69297/100000 [00:49<00:22, 1359.62it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4836429680585863\n"]},{"name":"stderr","output_type":"stream","text":[" 70%|███████   | 70226/100000 [00:50<00:22, 1307.59it/s]"]},{"name":"stdout","output_type":"stream","text":["2.502310041189194\n"]},{"name":"stderr","output_type":"stream","text":[" 71%|███████   | 71227/100000 [00:51<00:25, 1140.77it/s]"]},{"name":"stdout","output_type":"stream","text":["2.491058703660965\n"]},{"name":"stderr","output_type":"stream","text":[" 72%|███████▏  | 72148/100000 [00:52<00:22, 1235.35it/s]"]},{"name":"stdout","output_type":"stream","text":["2.479574960231781\n"]},{"name":"stderr","output_type":"stream","text":[" 73%|███████▎  | 73176/100000 [00:52<00:22, 1190.95it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4972022359371184\n"]},{"name":"stderr","output_type":"stream","text":[" 74%|███████▍  | 74181/100000 [00:53<00:21, 1213.59it/s]"]},{"name":"stdout","output_type":"stream","text":["2.482326678156853\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 75208/100000 [00:54<00:19, 1272.45it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4959389588832854\n"]},{"name":"stderr","output_type":"stream","text":[" 76%|███████▌  | 76188/100000 [00:55<00:19, 1219.37it/s]"]},{"name":"stdout","output_type":"stream","text":["2.486864819765091\n"]},{"name":"stderr","output_type":"stream","text":[" 77%|███████▋  | 77202/100000 [00:56<00:18, 1263.65it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4774719977378847\n"]},{"name":"stderr","output_type":"stream","text":[" 78%|███████▊  | 78270/100000 [00:57<00:16, 1315.29it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4838241155147553\n"]},{"name":"stderr","output_type":"stream","text":[" 79%|███████▉  | 79187/100000 [00:57<00:16, 1277.58it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4785149207115174\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|████████  | 80251/100000 [00:58<00:15, 1286.63it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4865104212760927\n"]},{"name":"stderr","output_type":"stream","text":[" 81%|████████  | 81206/100000 [00:59<00:13, 1387.65it/s]"]},{"name":"stdout","output_type":"stream","text":["2.476597564458847\n"]},{"name":"stderr","output_type":"stream","text":[" 82%|████████▏ | 82144/100000 [01:00<00:13, 1305.11it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4878266949653627\n"]},{"name":"stderr","output_type":"stream","text":[" 83%|████████▎ | 83246/100000 [01:01<00:12, 1318.32it/s]"]},{"name":"stdout","output_type":"stream","text":["2.476090267419815\n"]},{"name":"stderr","output_type":"stream","text":[" 84%|████████▍ | 84178/100000 [01:01<00:12, 1284.22it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4784894309043883\n"]},{"name":"stderr","output_type":"stream","text":[" 85%|████████▌ | 85240/100000 [01:02<00:11, 1321.41it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4841683148145677\n"]},{"name":"stderr","output_type":"stream","text":[" 86%|████████▌ | 86181/100000 [01:03<00:10, 1322.45it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4796539931297303\n"]},{"name":"stderr","output_type":"stream","text":[" 87%|████████▋ | 87117/100000 [01:04<00:10, 1246.73it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4650667282342913\n"]},{"name":"stderr","output_type":"stream","text":[" 88%|████████▊ | 88150/100000 [01:04<00:09, 1226.49it/s]"]},{"name":"stdout","output_type":"stream","text":["2.47642094707489\n"]},{"name":"stderr","output_type":"stream","text":[" 89%|████████▉ | 89199/100000 [01:05<00:08, 1273.67it/s]"]},{"name":"stdout","output_type":"stream","text":["2.460031349778175\n"]},{"name":"stderr","output_type":"stream","text":[" 90%|█████████ | 90141/100000 [01:06<00:07, 1326.89it/s]"]},{"name":"stdout","output_type":"stream","text":["2.475660652637482\n"]},{"name":"stderr","output_type":"stream","text":[" 91%|█████████ | 91204/100000 [01:07<00:06, 1343.52it/s]"]},{"name":"stdout","output_type":"stream","text":["2.462677534222603\n"]},{"name":"stderr","output_type":"stream","text":[" 92%|█████████▏| 92141/100000 [01:08<00:05, 1317.56it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4712490777969363\n"]},{"name":"stderr","output_type":"stream","text":[" 93%|█████████▎| 93112/100000 [01:08<00:05, 1325.92it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4716665992736817\n"]},{"name":"stderr","output_type":"stream","text":[" 94%|█████████▍| 94181/100000 [01:09<00:04, 1320.71it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4686378293037414\n"]},{"name":"stderr","output_type":"stream","text":[" 95%|█████████▌| 95248/100000 [01:10<00:03, 1310.77it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4689007756710053\n"]},{"name":"stderr","output_type":"stream","text":[" 96%|█████████▌| 96165/100000 [01:11<00:02, 1287.71it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4674968681335447\n"]},{"name":"stderr","output_type":"stream","text":[" 97%|█████████▋| 97254/100000 [01:12<00:02, 1303.16it/s]"]},{"name":"stdout","output_type":"stream","text":["2.466150576233864\n"]},{"name":"stderr","output_type":"stream","text":[" 98%|█████████▊| 98215/100000 [01:12<00:01, 1300.67it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4653475551605224\n"]},{"name":"stderr","output_type":"stream","text":[" 99%|█████████▉| 99133/100000 [01:13<00:00, 1163.61it/s]"]},{"name":"stdout","output_type":"stream","text":["2.4644364156723024\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100000/100000 [01:14<00:00, 1345.82it/s]\n"]}],"source":["# Let's train this model\n","import torch.optim as optim\n","from torch.nn import CrossEntropyLoss\n","from tqdm import tqdm\n","\n","# Hyperparameters\n","learning_rate = 0.01\n","momentum = 0.9\n","batch_size = 32\n","context_length = 8 # Is not0 really taken into consideration here because we are using a bigram model\n","num_iterations = 100000\n","device = \"cpu\"\n","\n","# Get the optimizer\n","optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n","# Get loss function\n","loss_fn = CrossEntropyLoss()\n","# Send the model to the device\n","model.to(device)\n","\n","loss_train = 0 \n","# Start training loop\n","for i in tqdm(range(num_iterations)):\n","    # Get a batch\n","    inputs, targets = get_batch(train_data, batch_size=8, context_length=8)\n","    # Send the inputs and targets to device\n","    inputs = inputs.to(device)\n","    targets = targets.to(device)\n","    # Get the predictions\n","    predictions = model(inputs)\n","    # We have to reshape the predictions and the targets to use cross entropy \n","    B, L, C = predictions.shape\n","    predictions = predictions.view(B*L, C)\n","    targets = targets.view(B*L)\n","    # Compute the loss\n","    loss = loss_fn(predictions, targets)\n","    # Accumulate the loss\n","    loss_train += loss.item()\n","    # Zero the gradients\n","    optimizer.zero_grad()\n","    # Compute the gradients\n","    loss.backward()\n","    # Update the parameters\n","    optimizer.step()\n","\n","    if (i%1000)==0 and i!=0:\n","        print(loss_train/1000)\n","        loss_train = 0"]},{"cell_type":"code","execution_count":152,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","“IqxB28jéYo ind the en, f Brmed serthe beif ton whe ring s orifi3]Yot e ird, the f t sut N5A03Zo wan\n"]}],"source":["# Let's try to generate\n","inputs_generate = torch.zeros((1,1), dtype=torch.long, device=device)\n","print(decode_char(model.generate(inputs_generate, max_new_tokens=100)[0].tolist()))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Ok, enough for the bigramLM, let's try to switch to some better architecture (that take into consideration the context)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Let's try with an RNN\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class RNNLM(nn.Module):\n","    def __init__(self, vocab_size, hidden_size=32):\n","        super(RNNLM, self).__init__()\n","        self.embedding_table = nn.Embedding(vocab_size, hidden_size)\n","        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n","        # Classifier to predict the next token\n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","    \n","    def forward(self, inputs):\n","        # Embed the inputs\n","        embeddings = self.embedding_table(inputs)\n","        # Pass the embeddings through the RNN\n","        outputs, _ = self.rnn(embeddings)\n","        # Pass the outputs through the classifier\n","        logits = self.linear(outputs)\n","        \n","        return logits\n","\n","    def generate(self, inputs, max_new_tokens=10):\n","        # Generate next tokens given the inputs (B, L, C)\n","        for _ in range(max_new_tokens):\n","            # Embed the inputs\n","            embeddings = self.embedding_table(inputs)\n","            # Feed the RNN with the embeddings \n","            _, hidden = self.rnn(embeddings)\n","            # Remove the first dimension (get only the last hidden state for each element in the batch)\n","            hidden = hidden.squeeze(0) \n","            # Project to the output classes\n","            logits = self.linear(hidden)\n","            # Convert to probabilities\n","            probs = F.softmax(logits, dim=1)\n","            # Get the next token\n","            next_token = torch.multinomial(probs, num_samples=1)\n","            # Concatenate to input\n","            inputs = torch.cat((inputs, next_token), dim=1)\n","            \n","        return inputs"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","]lRlhq2Fti}eéw…a[j8E‘Oiê\n","‘v;Q.9/IuSAo?”rAcK/hOC2…9rJêjDur;[‘/u\n","z:cK5U0;qWN(/B-dCVUQw21ko)IlH7qNNpO,K\n"]}],"source":["model = RNNLM(len(chars))\n","# Get a batch \n","inputs, targets = get_batch(train_data, batch_size=8, context_length=8)\n","\n","inputs_generate = torch.zeros((1,1), dtype=torch.long)\n","print(decode_char(model.generate(inputs_generate, max_new_tokens=100)[0].tolist()))"]},{"cell_type":"code","execution_count":155,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  1%|          | 1084/100000 [00:02<03:05, 532.19it/s]"]},{"name":"stdout","output_type":"stream","text":["2.687602298617363\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 2096/100000 [00:04<03:10, 513.64it/s]"]},{"name":"stdout","output_type":"stream","text":["2.3251679151058195\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 3095/100000 [00:06<03:09, 511.38it/s]"]},{"name":"stdout","output_type":"stream","text":["2.254286504745483\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 4087/100000 [00:08<03:09, 507.00it/s]"]},{"name":"stdout","output_type":"stream","text":["2.1959966259002686\n"]},{"name":"stderr","output_type":"stream","text":["  5%|▌         | 5085/100000 [00:10<03:23, 466.18it/s]"]},{"name":"stdout","output_type":"stream","text":["2.1667426545619963\n"]},{"name":"stderr","output_type":"stream","text":["  6%|▌         | 6050/100000 [00:12<03:03, 512.02it/s]"]},{"name":"stdout","output_type":"stream","text":["2.1494359769821165\n"]},{"name":"stderr","output_type":"stream","text":["  7%|▋         | 7061/100000 [00:14<03:23, 456.61it/s]"]},{"name":"stdout","output_type":"stream","text":["2.136654736161232\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 8046/100000 [00:16<03:41, 414.33it/s]"]},{"name":"stdout","output_type":"stream","text":["2.104321906924248\n"]},{"name":"stderr","output_type":"stream","text":["  9%|▉         | 9071/100000 [00:19<03:01, 499.94it/s]"]},{"name":"stdout","output_type":"stream","text":["2.101077096223831\n"]},{"name":"stderr","output_type":"stream","text":[" 10%|█         | 10081/100000 [00:21<02:51, 525.48it/s]"]},{"name":"stdout","output_type":"stream","text":["2.100099830508232\n"]},{"name":"stderr","output_type":"stream","text":[" 11%|█         | 11056/100000 [00:22<02:49, 524.90it/s]"]},{"name":"stdout","output_type":"stream","text":["2.086010811328888\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 12090/100000 [00:24<02:50, 515.02it/s]"]},{"name":"stdout","output_type":"stream","text":["2.073609950423241\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 13100/100000 [00:26<02:55, 495.90it/s]"]},{"name":"stdout","output_type":"stream","text":["2.072742078304291\n"]},{"name":"stderr","output_type":"stream","text":[" 14%|█▍        | 14102/100000 [00:28<02:49, 505.64it/s]"]},{"name":"stdout","output_type":"stream","text":["2.0765683393478396\n"]},{"name":"stderr","output_type":"stream","text":[" 15%|█▌        | 15066/100000 [00:30<02:53, 489.28it/s]"]},{"name":"stdout","output_type":"stream","text":["2.0692541717290878\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|█▌        | 16076/100000 [00:32<02:48, 498.56it/s]"]},{"name":"stdout","output_type":"stream","text":["2.065125738620758\n"]},{"name":"stderr","output_type":"stream","text":[" 17%|█▋        | 17041/100000 [00:34<02:44, 505.08it/s]"]},{"name":"stdout","output_type":"stream","text":["2.051761164903641\n"]},{"name":"stderr","output_type":"stream","text":[" 18%|█▊        | 18098/100000 [00:36<02:40, 510.97it/s]"]},{"name":"stdout","output_type":"stream","text":["2.0528037312030794\n"]},{"name":"stderr","output_type":"stream","text":[" 19%|█▉        | 19075/100000 [00:38<02:32, 530.24it/s]"]},{"name":"stdout","output_type":"stream","text":["2.0541995549201966\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|██        | 20067/100000 [00:40<02:25, 548.89it/s]"]},{"name":"stdout","output_type":"stream","text":["2.0574321101903914\n"]},{"name":"stderr","output_type":"stream","text":[" 21%|██        | 21080/100000 [00:42<02:31, 521.77it/s]"]},{"name":"stdout","output_type":"stream","text":["2.053194353222847\n"]},{"name":"stderr","output_type":"stream","text":[" 22%|██▏       | 22070/100000 [00:44<02:58, 435.85it/s]"]},{"name":"stdout","output_type":"stream","text":["2.0554324977397918\n"]},{"name":"stderr","output_type":"stream","text":[" 23%|██▎       | 23085/100000 [00:46<02:27, 520.26it/s]"]},{"name":"stdout","output_type":"stream","text":["2.0356800277233122\n"]},{"name":"stderr","output_type":"stream","text":[" 24%|██▍       | 24106/100000 [00:48<02:22, 531.52it/s]"]},{"name":"stdout","output_type":"stream","text":["2.037048539638519\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▌       | 25075/100000 [00:50<02:21, 530.06it/s]"]},{"name":"stdout","output_type":"stream","text":["2.04324201798439\n"]},{"name":"stderr","output_type":"stream","text":[" 26%|██▌       | 26088/100000 [00:52<02:22, 516.91it/s]"]},{"name":"stdout","output_type":"stream","text":["2.032268530011177\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 27049/100000 [00:54<03:18, 367.76it/s]"]},{"name":"stdout","output_type":"stream","text":["2.037448873281479\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 28054/100000 [00:56<02:57, 404.23it/s]"]},{"name":"stdout","output_type":"stream","text":["2.0309792625904084\n"]},{"name":"stderr","output_type":"stream","text":[" 29%|██▉       | 29035/100000 [00:59<04:10, 283.62it/s]"]},{"name":"stdout","output_type":"stream","text":["2.036928208231926\n"]},{"name":"stderr","output_type":"stream","text":[" 30%|███       | 30061/100000 [01:02<02:45, 423.57it/s]"]},{"name":"stdout","output_type":"stream","text":["2.025613047361374\n"]},{"name":"stderr","output_type":"stream","text":[" 31%|███       | 31091/100000 [01:04<02:21, 487.73it/s]"]},{"name":"stdout","output_type":"stream","text":["2.0270265258550646\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 32072/100000 [01:06<02:25, 468.20it/s]"]},{"name":"stdout","output_type":"stream","text":["2.02640233874321\n"]},{"name":"stderr","output_type":"stream","text":[" 33%|███▎      | 33049/100000 [01:08<02:31, 441.68it/s]"]},{"name":"stdout","output_type":"stream","text":["2.0232292529344558\n"]},{"name":"stderr","output_type":"stream","text":[" 34%|███▍      | 34085/100000 [01:11<02:46, 395.49it/s]"]},{"name":"stdout","output_type":"stream","text":["2.02665888440609\n"]},{"name":"stderr","output_type":"stream","text":[" 35%|███▌      | 35053/100000 [01:13<02:53, 374.23it/s]"]},{"name":"stdout","output_type":"stream","text":["2.019586059808731\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 36079/100000 [01:16<02:34, 413.97it/s]"]},{"name":"stdout","output_type":"stream","text":["2.0217912064790724\n"]},{"name":"stderr","output_type":"stream","text":[" 37%|███▋      | 37047/100000 [01:18<02:55, 358.48it/s]"]},{"name":"stdout","output_type":"stream","text":["2.0225897825956345\n"]},{"name":"stderr","output_type":"stream","text":[" 38%|███▊      | 38031/100000 [01:21<02:50, 363.93it/s]"]},{"name":"stdout","output_type":"stream","text":["2.020528027653694\n"]},{"name":"stderr","output_type":"stream","text":[" 39%|███▉      | 39052/100000 [01:23<02:24, 421.90it/s]"]},{"name":"stdout","output_type":"stream","text":["2.0181281260252\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|████      | 40043/100000 [01:26<02:52, 346.82it/s]"]},{"name":"stdout","output_type":"stream","text":["2.033061379313469\n"]},{"name":"stderr","output_type":"stream","text":[" 41%|████      | 41074/100000 [01:28<02:27, 398.20it/s]"]},{"name":"stdout","output_type":"stream","text":["2.0177590909004213\n"]},{"name":"stderr","output_type":"stream","text":[" 42%|████▏     | 42064/100000 [01:31<02:35, 371.97it/s]"]},{"name":"stdout","output_type":"stream","text":["2.0046874356269835\n"]},{"name":"stderr","output_type":"stream","text":[" 43%|████▎     | 43035/100000 [01:33<02:23, 397.70it/s]"]},{"name":"stdout","output_type":"stream","text":["2.016550761461258\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▍     | 44075/100000 [01:36<02:08, 434.23it/s]"]},{"name":"stdout","output_type":"stream","text":["2.018061314821243\n"]},{"name":"stderr","output_type":"stream","text":[" 45%|████▌     | 45085/100000 [01:38<02:11, 417.36it/s]"]},{"name":"stdout","output_type":"stream","text":["2.023320929169655\n"]},{"name":"stderr","output_type":"stream","text":[" 46%|████▌     | 46045/100000 [01:40<02:21, 381.83it/s]"]},{"name":"stdout","output_type":"stream","text":["2.01519689309597\n"]},{"name":"stderr","output_type":"stream","text":[" 47%|████▋     | 47056/100000 [01:43<02:18, 383.14it/s]"]},{"name":"stdout","output_type":"stream","text":["2.021000809431076\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 48068/100000 [01:46<02:17, 376.70it/s]"]},{"name":"stdout","output_type":"stream","text":["2.016027408838272\n"]},{"name":"stderr","output_type":"stream","text":[" 49%|████▉     | 49044/100000 [01:48<02:08, 395.80it/s]"]},{"name":"stdout","output_type":"stream","text":["2.0172686911821365\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 50101/100000 [01:50<01:47, 465.27it/s]"]},{"name":"stdout","output_type":"stream","text":["2.0220866557359694\n"]},{"name":"stderr","output_type":"stream","text":[" 51%|█████     | 51078/100000 [01:52<01:34, 516.31it/s]"]},{"name":"stdout","output_type":"stream","text":["2.0131549764871597\n"]},{"name":"stderr","output_type":"stream","text":[" 52%|█████▏    | 52101/100000 [01:54<01:28, 543.91it/s]"]},{"name":"stdout","output_type":"stream","text":["2.0178176679611206\n"]},{"name":"stderr","output_type":"stream","text":[" 53%|█████▎    | 52978/100000 [01:56<01:25, 547.25it/s]"]}],"source":["# Let's train this model\n","import torch.optim as optim\n","from torch.nn import CrossEntropyLoss\n","from tqdm import tqdm\n","\n","# Hyperparameters\n","learning_rate = 0.01\n","momentum = 0.9\n","batch_size = 32\n","context_length = 8 # Here it is important because we are using an RNN\n","hidden_size = 32\n","num_iterations = 100000\n","device = \"cpu\"\n","\n","# Create the model \n","model = RNNLM(len(chars), hidden_size=hidden_size)\n","# Send the model to the device\n","model.to(device)\n","\n","# Get the optimizer\n","optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n","# Get loss function\n","loss_fn = CrossEntropyLoss()\n","\n","\n","# Start training loop\n","loss_train = 0\n","for i in tqdm(range(num_iterations)):\n","    # Get a batch\n","    inputs, targets = get_batch(train_data, batch_size=8, context_length=8)\n","    # Send the inputs and targets to device\n","    inputs = inputs.to(device)\n","    targets = targets.to(device)\n","    # Get the predictions\n","    predictions = model(inputs)\n","    # We have to reshape the predictions and the targets to use cross entropy \n","    B, L, C = predictions.shape\n","    predictions = predictions.view(B*L, C)\n","    targets = targets.view(B*L)\n","    # Compute the loss\n","    loss = loss_fn(predictions, targets)\n","    # Accumulate the loss\n","    loss_train += loss.item()\n","    # Zero the gradients\n","    optimizer.zero_grad()\n","    # Compute the gradients\n","    loss.backward()\n","    # Update the parameters\n","    optimizer.step()\n","\n","    if (i%1000)==0 and i!=0:\n","        print(loss_train/1000)\n","        loss_train = 0"]},{"cell_type":"code","execution_count":198,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","“The herisses to said, bing tridane say as calmeed. Gord,s to up. HEim besess her hersesserselped o\n"]}],"source":["inputs_generate = torch.zeros((1,1), dtype=torch.long, device=device)\n","print(decode_char(model.generate(inputs_generate, max_new_tokens=100)[0].tolist()))"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn \n","import torch.nn.functional as F \n","# Let's try with an RNN\n","\n","class GRULM(nn.Module):\n","    def __init__(self, vocab_size, hidden_size=32):\n","        super(GRULM, self).__init__()\n","        self.embedding_table = nn.Embedding(vocab_size, hidden_size)\n","        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n","        # Classifier to predict the next token\n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","    \n","    def forward(self, inputs):\n","        # Embed the inputs\n","        embeddings = self.embedding_table(inputs)\n","        # Pass the embeddings through the RNN\n","        outputs, _ = self.rnn(embeddings)\n","        # Pass the outputs through the classifier\n","        logits = self.linear(outputs)\n","        \n","        return logits\n","\n","    def generate(self, inputs, max_new_tokens=10):\n","        # Generate next tokens given the inputs (B, L, C)\n","        for _ in range(max_new_tokens):\n","            # Embed the inputs\n","            embeddings = self.embedding_table(inputs)\n","            # Feed the RNN with the embeddings \n","            _, hidden = self.rnn(embeddings)\n","            # Remove the first dimension (get only the last hidden state for each element in the batch)\n","            hidden = hidden.squeeze(0) \n","            # Project to the output classes\n","            logits = self.linear(hidden)\n","            # Convert to probabilities\n","            probs = F.softmax(logits, dim=1)\n","            # Get the next token\n","            next_token = torch.multinomial(probs, num_samples=1)\n","            # Concatenate to input\n","            inputs = torch.cat((inputs, next_token), dim=1)\n","            \n","        return inputs"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","!dxR6X(EjfG9!mD[s-ai3]Vjy1UnopoCM‘muQY),XRM8VP“d:Lzs.n’’(.6C25é}xJ,tO2l.5{;!nE0!VN/.IjbagTC25HrN[xSd\n"]}],"source":["model = GRULM(len(chars))\n","\n","inputs_generate = torch.zeros((1,1), dtype=torch.long)\n","print(decode(model.generate(inputs_generate, max_new_tokens=100)[0].tolist()))"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  1%|          | 1052/100000 [00:04<06:26, 256.26it/s]"]},{"name":"stdout","output_type":"stream","text":["2.843295238018036\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 1979/100000 [00:08<07:12, 226.38it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[16], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     33\u001b[0m \u001b[39m# Get the predictions\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m predictions \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     35\u001b[0m \u001b[39m# We have to reshape the predictions and the targets to use cross entropy \u001b[39;00m\n\u001b[0;32m     36\u001b[0m B, L, C \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39mshape\n","File \u001b[1;32mc:\\Users\\Melgani\\anaconda3\\envs\\newnlpt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[1;32mIn[14], line 18\u001b[0m, in \u001b[0;36mGRULM.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     16\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_table(inputs)\n\u001b[0;32m     17\u001b[0m \u001b[39m# Pass the embeddings through the RNN\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m outputs, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(embeddings)\n\u001b[0;32m     19\u001b[0m \u001b[39m# Pass the outputs through the classifier\u001b[39;00m\n\u001b[0;32m     20\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(outputs)\n","File \u001b[1;32mc:\\Users\\Melgani\\anaconda3\\envs\\newnlpt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Melgani\\anaconda3\\envs\\newnlpt\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:955\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    954\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 955\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    956\u001b[0m                      \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    957\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    958\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mgru(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    959\u001b[0m                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Let's train this model\n","import torch.optim as optim\n","from torch.nn import CrossEntropyLoss\n","from tqdm import tqdm\n","\n","# Hyperparameters\n","learning_rate = 0.01\n","momentum = 0.9\n","batch_size = 32\n","context_length = 8 # Here it is important because we are using an RNN\n","hidden_size = 32\n","num_iterations = 100000\n","device = \"cpu\"\n","\n","# Get the model \n","model = GRULM(len(chars))\n","# Send the model to the device\n","model.to(device)\n","\n","# Get the optimizer\n","optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n","# Get loss function\n","loss_fn = CrossEntropyLoss()\n","\n","# Start training loop\n","loss_train = 0\n","for i in tqdm(range(num_iterations)):\n","    # Get a batch\n","    inputs, targets = get_batch(train_data, batch_size=8, context_length=8)\n","    # Send the inputs and targets to device\n","    inputs = inputs.to(device)\n","    targets = targets.to(device)\n","    # Get the predictions\n","    predictions = model(inputs)\n","    # We have to reshape the predictions and the targets to use cross entropy \n","    B, L, C = predictions.shape\n","    predictions = predictions.view(B*L, C)\n","    targets = targets.view(B*L)\n","    # Compute the loss\n","    loss = loss_fn(predictions, targets)\n","    # Accumulate the loss\n","    loss_train += loss.item()\n","    # Zero the gradients\n","    optimizer.zero_grad()\n","    # Compute the gradients\n","    loss.backward()\n","    # Update the parameters\n","    optimizer.step()\n","\n","    if (i%1000)==0 and i!=0:\n","        print(loss_train/1000)\n","        loss_train = 0"]},{"cell_type":"code","execution_count":207,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","“They me of of the trooofre togstered her. A chil?\n","\n","The’ld the was of they kie her knushing, godge-\n"]}],"source":["inputs_generate = torch.zeros((1,1), dtype=torch.long, device=device)\n","print(decode(model.generate(inputs_generate, max_new_tokens=100)[0].tolist()))"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training with context length:  512  and hidden size:  256\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 1047/100000 [00:13<19:30, 84.53it/s]"]}],"source":["# Do the study with different parameters and plot all the results \n","# RNN \n","import torch.optim as optim\n","from torch.nn import CrossEntropyLoss\n","from tqdm import tqdm\n","\n","# Use pickle to save the loss array\n","import pickle\n","\n","# Hyperparameters\n","learning_rate = 0.01\n","momentum = 0.9\n","batch_size = 32\n","context_length = [512] # Here it is important because we are using an RNN\n","hidden_size = [256]\n","num_iterations = 100000\n","device = \"cuda:1\"\n","\n","# Start training loop\n","for c in context_length:\n","    for h in hidden_size:\n","        print(\"Training with context length: \", c, \" and hidden size: \", h)\n","        # Get the model \n","        model = RNNLM(len(chars), hidden_size=h)\n","        # Send the model to the device\n","        model.to(device)\n","        # Get the optimizer\n","        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n","        # Get loss function\n","        loss_fn = CrossEntropyLoss()\n","        \n","        loss_train = 0\n","        loss_array = []\n","        for i in tqdm(range(num_iterations)):\n","            # Get a batch\n","            inputs, targets = get_batch(train_data, batch_size=batch_size, context_length=c)\n","            # Send the inputs and targets to device\n","            inputs = inputs.to(device)\n","            targets = targets.to(device)\n","            # Get the predictions\n","            predictions = model(inputs)\n","            # We have to reshape the predictions and the targets to use cross entropy \n","            B, L, C = predictions.shape\n","            predictions = predictions.view(B*L, C)\n","            targets = targets.view(B*L)\n","            # Compute the loss\n","            loss = loss_fn(predictions, targets)\n","            # Accumulate the loss\n","            loss_train += loss.item()\n","            # Zero the gradients\n","            optimizer.zero_grad()\n","            # Compute the gradients\n","            loss.backward()\n","            # Update the parameters\n","            optimizer.step()\n","\n","            if (i%1000)==0 and i!=0:\n","                loss_array.append(loss_train/1000)\n","                loss_train = 0\n","\n","        # Save the loss list \n","        with open(\"loss_rnn_context\"+str(c)+\"_hiddensize\"+str(h)+\".pkl\", \"wb\") as file:\n","            pickle.dump(loss_array, file)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","“Why would help me to it.” Go rode his helm. Maryon’s happing third Small king was color-things dark suit of the aware old forbug. Ser Boros strode by log twists than Harbel as an silver shield she had selded him at a bejoies. The alehants rushed in his sand’s throat. Bran shook his bone-mooking ovention with her own pair hottering downstrings against you, no matter rock of animals. A Tennmaid the Skirlings, Sansa’s heart where whenever for the varrows, Mucah… the wagon of Clegane’s stone. There is a cumpless come sewn square as the blew back in the truth. The thinner stewled into the get. On a tink of field Lady Mathis pointed displeastriamondy. There would hammered a troth the seal as they listen, that mox and offendage will be all it would not attendly not as us on the stories, and little time summers.” He raved stripping off his endlessoard were screaming beneath him that the treacheroom, there were they were safe into the songs.\n","\n","“What,” Robb Stark Tallhart. Sansa told herself ma\n"]}],"source":["inputs_generate = torch.zeros((1,1), dtype=torch.long, device=device)\n","print(decode_char(model.generate(inputs_generate, max_new_tokens=1000)[0].tolist()))"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# Save the model\n","torch.save(model.state_dict(), \"model_grurnn_context\"+str(c)+\"_hiddensize\"+str(h)+\".pt\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMBMOdwLn83pc4CCsHogsnV","mount_file_id":"18d5LXhD-HFqZNbPwpVHP7Lwh6PzFPS3p","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"newnlpt","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"8ac9e49d44b760bd93688ef99a9561cf0cd875fa001e5a2bed6c9672b8b9b17d"}}},"nbformat":4,"nbformat_minor":0}
